{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vocational-assessment",
   "metadata": {},
   "source": [
    "# 手写数字识别\n",
    "\n",
    "\n",
    "### 问题：分类问题（10类）\n",
    "\n",
    "### 输入：灰度图像（28×28个像素）\n",
    "\n",
    "### 输出：分类0-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78c8d8",
   "metadata": {},
   "source": [
    "# 0.超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513206fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "每次在训练集中提取64张图像进行批量化训练，目的是提高训练速度。\n",
    "就好比搬砖，一次搬一块砖头的效率肯定要比一次能搬64块要低得多\n",
    "\"\"\"\n",
    "BATCH_SIZE = 64\n",
    "#学习率，学习率一般为0.01，0.1等等较小的数，为了在梯度下降求解时避免错过最优解\n",
    "LR = 0.001\n",
    "\"\"\"\n",
    "EPOCH 假如现在我有1000张训练图像，因为每次训练是64张，\n",
    "每当我1000张图像训练完就是一个EPOCH，训练多少个EPOCH自己决定\n",
    "\"\"\"\n",
    "EPOCH = 1\n",
    "\"\"\"\n",
    "现在我要训练的训练集是系统自带的，需要先下载数据集，\n",
    "当DOWNLOAD_MNIST为True是表示学要下载数据集，一但下载完，保存\n",
    "然后这个参数就可以改为False，表示不用再次下载\n",
    "\"\"\"\n",
    "DOWNLOAD_MNIST = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-great",
   "metadata": {},
   "source": [
    "# 1.导入数据\n",
    "\n",
    "### 原始数据（来自keras.datasets.mnist）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monetary-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动导入数据\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() #mnist.load_data('路径')为下载并保存数据集位置，默认位置在C:\\Users\\管理员\\.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb8fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# pytorch导入\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "#训练集\n",
    "# 读取\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist',\n",
    "    train = True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=DOWNLOAD_MNIST\n",
    ")\n",
    "\n",
    "# 划分\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2 )\n",
    "#每个batch_size的shape为[64, 1, 28, 28]\n",
    "print(\"样本\")\n",
    "print(train_data.train_data.shape)\n",
    "print(train_data.train_data[:3])\n",
    "print(\"标签\")\n",
    "print(train_data.train_labels.shape)\n",
    "print(train_data.train_labels[:3])\n",
    "\n",
    "\n",
    "\n",
    "# 测试集\n",
    "# 读取\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist',\n",
    "    train = False,\n",
    ")\n",
    "\n",
    "# 处理\n",
    "\n",
    "test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000]/255.0\n",
    "\"\"\"\n",
    "test_data.test_data中的shape为[10000, 28, 28]代表1w张图像，都是28x28，当时并未表明channels,因此在unsqueeze在1方向想加一个维度，\n",
    "则shape变为[10000, 1, 28, 28]，然后转化为tensor的float32类型，取1w张中的2000张，并且将其图片进行归一化处理，避免图像几何变换的影响\n",
    "\"\"\"\n",
    "#标签取前2000\n",
    "test_y = test_data.test_labels[:2000]\n",
    "\n",
    "print(\"样本\")\n",
    "print(test_x.shape)\n",
    "print(test_x[:3])\n",
    "print(\"标签\")\n",
    "print(test_y.shape)\n",
    "print(test_y[:3])\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-craps",
   "metadata": {},
   "source": [
    "# 2.创建自己的Datasets数据集\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset ,DataLoader\n",
    "\n",
    "\n",
    ">1.将数据转为tensor格式\n",
    ">>`数据 = torch.tensor(mumpy数据)` \n",
    "\n",
    "\n",
    ">2.数据处理\n",
    ">>图像数据处理：\n",
    ">>>**图像数据列表维度shape：[图像数量,通道维数,图像长像素,图像宽像素]**  \n",
    ">>>缺少通道维黑白图像处理:`图片样本data = Variable(torch.unsqueeze(图片样本data, dim=1), volatile=True).type(torch.FloatTensor)/255`  \n",
    ">>>数据类型转换:`数据变量 = 数据变量.type(torch.FloatTensor)`\n",
    "\n",
    ">>标签处理\n",
    ">>>转换为one-hot编码:`标签labels = utils.to_categorical(标签labels)`  \n",
    ">>>标签转换成long数据格式：`标签labels = 标签labels.long()`\n",
    "\n",
    ">3.创建数据集\n",
    ">>`数据集 = TensorDataset(样本data, 标签labels)`\n",
    "\n",
    ">4.加载数据集\n",
    ">>`train_loader = DataLoader(train_dataset, batch_size=120)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d962467",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 对分类标签y进行one-hot编码  utils.to_categorical(标签列表, num_classes=标签类别数, dtype='编码后标签格式')\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "print(\"编码前\")\n",
    "print(train_labels)\n",
    "\n",
    "train_labels = utils.to_categorical(train_labels)\n",
    "test_labels = utils.to_categorical(test_labels)\n",
    "\n",
    "print(\"编码后\")\n",
    "print(train_labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2900642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-8705a23add60>:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  train_images = Variable(torch.unsqueeze(train_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n",
      "<ipython-input-3-8705a23add60>:24: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  test_images = Variable(torch.unsqueeze(test_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "print(train_images)\n",
    "\n",
    "# 1.把数据转换成tensor格式\n",
    "train_images = torch.tensor(train_images)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "test_images = torch.tensor(test_images)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "print(train_images.shape)\n",
    "\n",
    "\n",
    "# 2. 数据处理\n",
    "# 将标签转换成long格式\n",
    "train_labels = train_labels.long()\n",
    "test_labels = test_labels.long()\n",
    "\n",
    "# 图像数据调整增加维度 [图片数, 长, 宽]->[图片数, 通道数, 长, 宽], 将数据转为tensor的Float格式\n",
    "train_images = Variable(torch.unsqueeze(train_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n",
    "test_images = Variable(torch.unsqueeze(test_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n",
    "\n",
    "print(train_images.shape)\n",
    "\n",
    "\n",
    "# 3.创建数据集\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "test_dataset = TensorDataset(test_images, test_labels)\n",
    "\n",
    "\n",
    "# 4.加载数据集\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader =DataLoader(test_dataset, batch_size=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-duration",
   "metadata": {},
   "source": [
    "# 3.构建网络\n",
    "\n",
    "```\n",
    "# 导入包\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义神经网络类\n",
    "class 自定义神经网络类名(nn.Module):\n",
    "    # 可学习参数的层（如全连接层、卷积层等）\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.自定义layer名1 = nn.layer层(参数)\n",
    "        self.自定义layer名2 = nn.layer层(参数)\n",
    "        self.自定义layer名3 = nn.Sequential(\n",
    "            nn.layer层(参数)，\n",
    "            nn.layer层(参数)\n",
    "        ）\n",
    "        \n",
    "    # 实现模型的功能，实现各个层之间的连接关系\n",
    "    # nn.functional实现不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)的构造\n",
    "    def forward(self, x):\n",
    "        x = self.自定义layer名1(x)\n",
    "        x = F.不可学习参数层(x)\n",
    "        x = self.自定义layer名2(x)\n",
    "        x = 不可学习参数层(自定义layer名3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 实例化神经网络\n",
    "model实例 = 自定义神经网络类名()\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model实例, '存储路径')\n",
    "\n",
    "# 加载模型\n",
    "model = torch.load(\"model.pth\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-marble",
   "metadata": {},
   "source": [
    "#### ·可学习layer\n",
    "\n",
    ">**卷积层**\n",
    ">>nn.Conv2d(\n",
    ">>&nbsp;&nbsp;&nbsp;  in_channels = 输入特征矩阵的深度,   \n",
    ">>&nbsp;&nbsp;&nbsp;  out_channels = 卷积核数|输出特征矩阵深度,   \n",
    ">>&nbsp;&nbsp;&nbsp;  kernel_size = (卷积核长, 卷积核宽),   \n",
    ">>&nbsp;&nbsp;&nbsp;  stride = 卷积框步长,   \n",
    ">>&nbsp;&nbsp;&nbsp;  padding = (填充上下行数, 填充左右列数),   \n",
    ">>&nbsp;&nbsp;&nbsp;  dilation = 卷积核元素之间的间距1,   \n",
    ">>&nbsp;&nbsp;&nbsp;  groups = 从输入通道到输出通道的阻塞连接数1,   \n",
    ">>&nbsp;&nbsp;&nbsp;  bias = 添加偏置T/F,   \n",
    ">>&nbsp;&nbsp;&nbsp;  padding_mode = '填充数字zeros'  \n",
    ">>)\n",
    "\n",
    ">>说明：\n",
    ">>>in_channels = 输入通道维的元素数  \n",
    ">>>图像(通道，图像长，图像宽)（黑白图像通道=1，RGB图像通道=3）   \n",
    ">>>out_channels = 提取特征数 = 输出特征矩阵深度 = 输出特征矩阵通道维\n",
    "\n",
    ">**全连接层**\n",
    ">>nn.Linear(in_features=每个输入样本的大小, out_features=每个输出样本的大小)  \n",
    ">>说明：\n",
    "\n",
    "\n",
    "#### ·不可学习layer\n",
    ">**将多维数据转成一维**\n",
    ">>x.view(x.size(0), -1)  \n",
    ">>说明：在卷积层转全连接层之间使用，用在forward(self, x)中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "binding-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 定义神经网络类\n",
    "class CNN(nn.Module):\n",
    "    # 可学习参数的层（如全连接层、卷积层等）\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 第一部分卷积层1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # 卷积层(输入通道维1，输出通道维16，卷积核3)\n",
    "            nn.Conv2d(1, 16, kernel_size=(3,3), stride=1, padding=1),  # 维度变换(1,28,28) （黑白图像1通道，长28像素，宽28像素）->(16,28,28) （16个卷积核提取16个特征通道，长28，宽28）图像边缘扩展，没被卷积抛去\n",
    "            # 激活函数\n",
    "            nn.ReLU(),\n",
    "            # 池化层\n",
    "            nn.MaxPool2d(2) # 维度变化(16,28,28)->(16,14,14)\n",
    "        )\n",
    "            \n",
    "        #第二部分卷积层2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # 卷积层(输入通道维1，输出通道维16，卷积核3)\n",
    "            nn.Conv2d(16, 32, kernel_size=(3,3), stride=1, padding=1),  # 维度变换(16,14,14)->(32,14,14)\n",
    "            # 激活函数\n",
    "            nn.ReLU(),\n",
    "            # 池化层\n",
    "            nn.MaxPool2d(2) # 维度变化(32,14,14)->(32,7,7)\n",
    "        )\n",
    "            \n",
    "        # 全连接层\n",
    "        self.out = nn.Linear(32*7*7, 10)\n",
    "            \n",
    "            \n",
    "        \n",
    "    # 实现模型的功能，实现各个层之间的连接关系\n",
    "    # nn.functional实现不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)的构造\n",
    "    def forward(self, x):\n",
    "        # 执行卷积层1 conv1\n",
    "        x = self.conv1(x)\n",
    "        # 执行卷积层2 conv2\n",
    "        x = self.conv2(x)\n",
    "        # 将图像数据转为1维\n",
    "        x = x.view(x.size(0),-1)\n",
    "        # 执行全连接层 out\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa53c7",
   "metadata": {},
   "source": [
    "# 4.训练模型\n",
    "\n",
    "(需要torchkeras支持)  \n",
    "import from torchkeras import summary,Model \n",
    "\n",
    "1.实例化模型  \n",
    "```\n",
    "model = Model(自定义神经网络类名())\n",
    "```\n",
    "\n",
    "2.编译模型\n",
    "```\n",
    "model.compile(loss_func = 损失函数,\n",
    "             optimizer= 优化方法,\n",
    "             metrics_dict={\"accuracy\":accuracy})\n",
    "```\n",
    "\n",
    "3.训练模型\n",
    "```\n",
    "dfhistory = model.fit(训练次数,train_loader, test_loader, log_step_freq=100) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b593b836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (net): CNN(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             160\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "            Conv2d-4           [-1, 32, 14, 14]           4,640\n",
      "              ReLU-5           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
      "            Linear-7                   [-1, 10]          15,690\n",
      "================================================================\n",
      "Total params: 20,490\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.002991\n",
      "Forward/backward pass size (MB): 0.323074\n",
      "Params size (MB): 0.078163\n",
      "Estimated Total Size (MB): 0.404228\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchkeras import summary,Model\n",
    "\n",
    "# 实例化模型\n",
    "model = Model(CNN())\n",
    "model = model.float()\n",
    "\n",
    "# 查看模型\n",
    "print(model)\n",
    "print(summary(model, input_shape=(1,28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71543cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(y_pred,y_true):\n",
    "    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data\n",
    "    return accuracy_score(y_true.numpy(),y_pred_cls.numpy())\n",
    "\n",
    "model.compile(loss_func = nn.CrossEntropyLoss(),\n",
    "             optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),\n",
    "             metrics_dict={\"accuracy\":accuracy})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57bbfa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n",
      "\n",
      "================================================================================2021-10-09 09:50:12\n",
      "{'step': 100, 'loss': 0.37, 'accuracy': 0.881}\n",
      "{'step': 200, 'loss': 0.25, 'accuracy': 0.922}\n",
      "{'step': 300, 'loss': 0.198, 'accuracy': 0.939}\n",
      "{'step': 400, 'loss': 0.176, 'accuracy': 0.945}\n",
      "{'step': 500, 'loss': 0.163, 'accuracy': 0.95}\n",
      "{'step': 600, 'loss': 0.151, 'accuracy': 0.954}\n",
      "{'step': 700, 'loss': 0.143, 'accuracy': 0.956}\n",
      "{'step': 800, 'loss': 0.136, 'accuracy': 0.959}\n",
      "{'step': 900, 'loss': 0.131, 'accuracy': 0.96}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   1   | 0.129 |  0.961   |  0.089   |    0.972     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2021-10-09 09:50:27\n",
      "{'step': 100, 'loss': 0.083, 'accuracy': 0.974}\n",
      "{'step': 200, 'loss': 0.081, 'accuracy': 0.975}\n",
      "{'step': 300, 'loss': 0.082, 'accuracy': 0.975}\n",
      "{'step': 400, 'loss': 0.083, 'accuracy': 0.975}\n",
      "{'step': 500, 'loss': 0.083, 'accuracy': 0.975}\n",
      "{'step': 600, 'loss': 0.084, 'accuracy': 0.975}\n",
      "{'step': 700, 'loss': 0.084, 'accuracy': 0.975}\n",
      "{'step': 800, 'loss': 0.084, 'accuracy': 0.975}\n",
      "{'step': 900, 'loss': 0.086, 'accuracy': 0.975}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   2   | 0.086 |  0.975   |  0.092   |    0.972     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2021-10-09 09:50:43\n",
      "{'step': 100, 'loss': 0.068, 'accuracy': 0.98}\n",
      "{'step': 200, 'loss': 0.068, 'accuracy': 0.979}\n",
      "{'step': 300, 'loss': 0.074, 'accuracy': 0.977}\n",
      "{'step': 400, 'loss': 0.077, 'accuracy': 0.977}\n",
      "{'step': 500, 'loss': 0.076, 'accuracy': 0.978}\n",
      "{'step': 600, 'loss': 0.073, 'accuracy': 0.978}\n",
      "{'step': 700, 'loss': 0.073, 'accuracy': 0.979}\n",
      "{'step': 800, 'loss': 0.073, 'accuracy': 0.979}\n",
      "{'step': 900, 'loss': 0.073, 'accuracy': 0.979}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   3   | 0.073 |  0.979   |  0.067   |    0.979     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2021-10-09 09:50:58\n",
      "Finished Training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfhistory = model.fit(3,train_loader, test_loader, log_step_freq=100) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e561eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型 .save('模型.h5')\n",
    "\n",
    "torch.save(model,'pytorch_number_model.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ff6c5",
   "metadata": {},
   "source": [
    "# 5.模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchkeras import summary,Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# 加载模型\n",
    "model = torch.load('pytorch_number_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9238dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片原始格式: (4, 28, 28)\n",
      "标签： [7 2 1 0]\n",
      "图片转换成tensor格式后： torch.Size([4, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 手动导入预测数据\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() \n",
    "\n",
    "# 取前4张图片\n",
    "test_images = test_images[:4]\n",
    "test_labels = test_labels[:4]\n",
    "print('图片原始格式:', test_images.shape)\n",
    "print('标签：', test_labels)\n",
    "\n",
    "# 图片格式转换成tensor格式\n",
    "test_images = torch.tensor(test_images)\n",
    "print('图片转换成tensor格式后：', test_images.shape)\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(test_images[i])\n",
    "plt.show\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a0208ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 28, 28])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "# 图片类型转换成FloatTensor\n",
    "test_images = test_images.type(torch.FloatTensor)/255\n",
    "\n",
    "print(test_images.shape)\n",
    "print(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72140cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "添加通道维度 torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 为图片格式添加通道维\n",
    "pic = test_images[0:4]\n",
    "\n",
    "pic = pic.reshape(4,1,28,28)\n",
    "print('添加通道维度', pic.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e81d8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -8.6000,  -4.5084,  -3.6726,   1.4685, -10.2292, -10.9920, -16.5648,\n",
      "          12.3982,  -9.8070,   1.2471],\n",
      "        [ -4.7192,   1.9776,  11.9519,  -4.1649, -11.9652, -17.6068,  -8.4505,\n",
      "          -5.1724,  -5.6687, -10.9413],\n",
      "        [ -5.2290,   9.6727,  -2.4976,  -5.1854,   1.4415,  -1.6183,  -2.0439,\n",
      "          -2.2289,  -0.5607,  -5.4894],\n",
      "        [ 14.4437, -15.4952,  -6.5456,  -6.5650, -10.6046, -10.7800,   3.8889,\n",
      "          -1.9229,  -4.5398,  -3.2224]], grad_fn=<AddmmBackward>)\n",
      "------\n",
      "torch.return_types.max(\n",
      "values=tensor([12.3982, 11.9519,  9.6727, 14.4437], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([7, 2, 1, 0]))\n"
     ]
    }
   ],
   "source": [
    "# 预测模式\n",
    "\n",
    "output = model(pic)\n",
    "print(output)\n",
    "\n",
    "print('------')\n",
    "# 按行找到一行内值最大的列号\n",
    "prediction = torch.max(output, dim=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe27fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 实际标签\n",
    "print(test_labels[0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587e9030",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
