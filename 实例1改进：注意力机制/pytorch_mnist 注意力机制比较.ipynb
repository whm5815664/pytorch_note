{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vocational-assessment",
   "metadata": {},
   "source": [
    "# 手写数字识别\n",
    "\n",
    "\n",
    "### 问题：分类问题（10类）\n",
    "\n",
    "### 输入：灰度图像（28×28个像素）\n",
    "\n",
    "### 输出：分类0-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78c8d8",
   "metadata": {},
   "source": [
    "# 0.超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513206fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "每次在训练集中提取64张图像进行批量化训练，目的是提高训练速度。\n",
    "就好比搬砖，一次搬一块砖头的效率肯定要比一次能搬64块要低得多\n",
    "\"\"\"\n",
    "BATCH_SIZE = 64\n",
    "#学习率，学习率一般为0.01，0.1等等较小的数，为了在梯度下降求解时避免错过最优解\n",
    "LR = 0.001\n",
    "\"\"\"\n",
    "EPOCH 假如现在我有1000张训练图像，因为每次训练是64张，\n",
    "每当我1000张图像训练完就是一个EPOCH，训练多少个EPOCH自己决定\n",
    "\"\"\"\n",
    "EPOCH = 1\n",
    "\"\"\"\n",
    "现在我要训练的训练集是系统自带的，需要先下载数据集，\n",
    "当DOWNLOAD_MNIST为True是表示学要下载数据集，一但下载完，保存\n",
    "然后这个参数就可以改为False，表示不用再次下载\n",
    "\"\"\"\n",
    "DOWNLOAD_MNIST = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-great",
   "metadata": {},
   "source": [
    "# 1.导入数据\n",
    "\n",
    "### 原始数据（来自keras.datasets.mnist）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monetary-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动导入数据\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() #mnist.load_data('路径')为下载并保存数据集位置，默认位置在C:\\Users\\管理员\\.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fb8fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# pytorch导入\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "#训练集\n",
    "# 读取\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist',\n",
    "    train = True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=DOWNLOAD_MNIST\n",
    ")\n",
    "\n",
    "# 划分\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2 )\n",
    "#每个batch_size的shape为[64, 1, 28, 28]\n",
    "print(\"样本\")\n",
    "print(train_data.train_data.shape)\n",
    "print(train_data.train_data[:3])\n",
    "print(\"标签\")\n",
    "print(train_data.train_labels.shape)\n",
    "print(train_data.train_labels[:3])\n",
    "\n",
    "\n",
    "\n",
    "# 测试集\n",
    "# 读取\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist',\n",
    "    train = False,\n",
    ")\n",
    "\n",
    "# 处理\n",
    "\n",
    "test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000]/255.0\n",
    "\"\"\"\n",
    "test_data.test_data中的shape为[10000, 28, 28]代表1w张图像，都是28x28，当时并未表明channels,因此在unsqueeze在1方向想加一个维度，\n",
    "则shape变为[10000, 1, 28, 28]，然后转化为tensor的float32类型，取1w张中的2000张，并且将其图片进行归一化处理，避免图像几何变换的影响\n",
    "\"\"\"\n",
    "#标签取前2000\n",
    "test_y = test_data.test_labels[:2000]\n",
    "\n",
    "print(\"样本\")\n",
    "print(test_x.shape)\n",
    "print(test_x[:3])\n",
    "print(\"标签\")\n",
    "print(test_y.shape)\n",
    "print(test_y[:3])\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-craps",
   "metadata": {},
   "source": [
    "# 2.创建自己的Datasets数据集\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset ,DataLoader\n",
    "\n",
    "\n",
    ">1.将数据转为tensor格式\n",
    ">>`数据 = torch.tensor(mumpy数据)` \n",
    "\n",
    "\n",
    ">2.数据处理\n",
    ">>图像数据处理：\n",
    ">>>**图像数据列表维度shape：[图像数量,通道维数,图像长像素,图像宽像素]**  \n",
    ">>>缺少通道维黑白图像处理:`图片样本data = Variable(torch.unsqueeze(图片样本data, dim=1), volatile=True).type(torch.FloatTensor)/255`  \n",
    ">>>数据类型转换:`数据变量 = 数据变量.type(torch.FloatTensor)`\n",
    "\n",
    ">>标签处理\n",
    ">>>转换为one-hot编码:`标签labels = utils.to_categorical(标签labels)`  \n",
    ">>>标签转换成long数据格式：`标签labels = 标签labels.long()`\n",
    "\n",
    ">3.创建数据集\n",
    ">>`数据集 = TensorDataset(样本data, 标签labels)`\n",
    "\n",
    ">4.加载数据集\n",
    ">>`train_loader = DataLoader(train_dataset, batch_size=120)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d962467",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# 对分类标签y进行one-hot编码  utils.to_categorical(标签列表, num_classes=标签类别数, dtype='编码后标签格式')\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "print(\"编码前\")\n",
    "print(train_labels)\n",
    "\n",
    "train_labels = utils.to_categorical(train_labels)\n",
    "test_labels = utils.to_categorical(test_labels)\n",
    "\n",
    "print(\"编码后\")\n",
    "print(train_labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2900642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-8705a23add60>:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  train_images = Variable(torch.unsqueeze(train_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n",
      "<ipython-input-3-8705a23add60>:24: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  test_images = Variable(torch.unsqueeze(test_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "print(train_images)\n",
    "\n",
    "# 1.把数据转换成tensor格式\n",
    "train_images = torch.tensor(train_images)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "test_images = torch.tensor(test_images)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "print(train_images.shape)\n",
    "\n",
    "\n",
    "# 2. 数据处理\n",
    "# 将标签转换成long格式\n",
    "train_labels = train_labels.long()\n",
    "test_labels = test_labels.long()\n",
    "\n",
    "# 图像数据调整增加维度 [图片数, 长, 宽]->[图片数, 通道数, 长, 宽], 将数据转为tensor的Float格式\n",
    "train_images = Variable(torch.unsqueeze(train_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n",
    "test_images = Variable(torch.unsqueeze(test_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n",
    "\n",
    "print(train_images.shape)\n",
    "\n",
    "\n",
    "# 3.创建数据集\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "test_dataset = TensorDataset(test_images, test_labels)\n",
    "\n",
    "\n",
    "# 4.加载数据集\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader =DataLoader(test_dataset, batch_size=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-duration",
   "metadata": {},
   "source": [
    "# 3.构建网络\n",
    "\n",
    "```python\n",
    "# 导入包\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义神经网络类\n",
    "class 自定义神经网络类名(nn.Module):\n",
    "    # 可学习参数的层（如全连接层、卷积层等）\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.自定义layer名1 = nn.layer层(参数)\n",
    "        self.自定义layer名2 = nn.layer层(参数)\n",
    "        self.自定义layer名3 = nn.Sequential(\n",
    "            nn.layer层(参数)，\n",
    "            nn.layer层(参数)\n",
    "        ）\n",
    "        \n",
    "    # 实现模型的功能，实现各个层之间的连接关系\n",
    "    # nn.functional实现不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)的构造\n",
    "    def forward(self, x):\n",
    "        x = self.自定义layer名1(x)\n",
    "        x = F.不可学习参数层(x)\n",
    "        x = self.自定义layer名2(x)\n",
    "        x = 不可学习参数层(自定义layer名3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 实例化神经网络\n",
    "model实例 = 自定义神经网络类名()\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model实例, '存储路径')\n",
    "\n",
    "# 加载模型\n",
    "model = torch.load(\"model.pth\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71b2bd5",
   "metadata": {},
   "source": [
    "### 3.1 注意力机制模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72731e7",
   "metadata": {},
   "source": [
    "#### （1）注意力机制1 eca注意力模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8f78e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eca注意力\n",
    "# -------------------------------------------- #\n",
    "# 实现方式：\n",
    "# 1、对输入进来的特征层进行全局平均池化。\n",
    "# 2、用1*1卷积代替2次全连接。\n",
    "# 3、取一次Sigmoid将值固定到0-1之间，此时我们获得了输入特征层每一个通道的权值（0-1之间）。\n",
    "# 4、在获得这个权值后，我们将这个权值乘上原输入特征层即可。\n",
    "# -------------------------------------------- #\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "\n",
    "class eca_block(nn.Module):\n",
    "    def __init__(self, channel, b=1, gamma=2):  # channel通道数：用于计算卷积核大小\n",
    "        super(eca_block, self).__init__()\n",
    "        # 通过输入通道数自适应计算卷积核大小\n",
    "        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))\n",
    "        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1\n",
    "\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # # 全局平均池化层\n",
    "        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False)  # 1*1卷积代替全连接\n",
    "        self.sigmoid = nn.Sigmoid()  # 获取每个特征点的权重\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.avg_pool(x)\n",
    "        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)\n",
    "        y = self.sigmoid(y)\n",
    "        return x * y.expand_as(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fb37fb",
   "metadata": {},
   "source": [
    "#### （2）注意力机制2 CBAM模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3310e2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CBAM = 通道注意力机制 + 空间注意力机制\n",
    "\n",
    "\n",
    "# 通道注意力机制\n",
    "# ------------------------------ #\n",
    "# 1.对输入进来的单个特征层，分别进行全局平均池化和全局最大池化（消除像素空间hw维度，保留通道维度c）\n",
    "# 2.对平均池化和最大池化的结果，利用共享的全连接层进行处理\n",
    "# 3.对处理后的两个结果进行相加，然后取一个sigmoid, 获得输入特征层每一个通道的权值（0-1之间）\n",
    "# 4.将这个权值乘上原输入特征层\n",
    "# ------------------------------ #\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channel, ratio=8):\n",
    "        super(ChannelAttention, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)  # 全局平均池化层,输出尺寸为[图片数b,通道数c,1,1]\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)  # 全局最大池化层,输出尺寸为[图片数b,通道数c,1,1]\n",
    "\n",
    "        # 共享的全连接\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channel, channel // ratio, False),  # 第一次全连接\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(channel // ratio, channel, False)  # 第二次全连接\n",
    "        )\n",
    "        # 可利用1x1卷积代替全连接\n",
    "        # self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)\n",
    "        # self.relu1 = nn.ReLU()\n",
    "        # self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)\n",
    "\n",
    "        # sigmoid层\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.size()  # 计算输入特征层的size：图片数b, 通道数c, 高h, 宽w\n",
    "        max_pool_out = self.max_pool(x).view([b, c])  # 1.全局最大池化，并转换格式为[图片数,通道数](消除图片像素hw维度)\n",
    "        avg_pool_out = self.avg_pool(x).view([b, c])  # 1.全局平均池化，并转换格式为[图片数,通道数](消除图片像素hw维度)\n",
    "\n",
    "        max_fc_out = self.fc(max_pool_out)  # 2.全局最大池化结果进行共享的全连接层进行处理\n",
    "        avg_fc_out = self.fc(avg_pool_out)  # 2.全局平均池化结果进行共享的全连接层进行处理\n",
    "\n",
    "        out = max_fc_out + avg_fc_out  # 3.对处理后的两个结果进行相加\n",
    "        out = self.sigmoid(out).view([b, c, 1, 1])  # 3. 然后取一个sigmoid，获得输入特征层每一个特征点的权值\n",
    "        return out\n",
    "\n",
    "        # avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))  # 全局平均池化->全连接\n",
    "        # max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))\n",
    "        # out = avg_out + max_out\n",
    "        # return self.sigmoid(out)\n",
    "\n",
    "\n",
    "\n",
    "# 空间注意力机制\n",
    "# ------------------------------ #\n",
    "# 1. 会对输入进来的特征层，在每一个特征点的通道上取最大值和平均值(图片中同一像素点上的不同维度平均值，消除通道维c，保留像素空间hw维)\n",
    "# 2. 将这两个结果进行一个堆叠，利用一次通道数为1的卷积调整通道数\n",
    "# 3. 取一个sigmoid获得输入特征层每一个特征点的权值（0-1之间）\n",
    "# 4. 将这个权值乘上原输入特征层\n",
    "# ------------------------------ #\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, kernel_size=7):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'  # 卷积核conv layer大小必须为3或7\n",
    "        padding = 3 if kernel_size == 7 else 1   # 当卷积核大小为7时，输入的每一条边补充3的层数；当卷积核大小为3时，输入的每一条边补充1的层数\n",
    "\n",
    "        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)  # 卷积：输入维度2[MaxPool,AvgPool]，输出维度1\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)    # 1.在每一个特征点的通道上取平均值(图片中同一像素点上的不同维度平均值) 结果消除通道c维度\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)  # 1.在每一个特征点的通道上取最大值(图片中同一像素点上的不同维度最大值) 结果消除通道c维度\n",
    "\n",
    "        x = torch.cat([avg_out, max_out], dim=1)  # 2.通道维度堆叠\n",
    "        x = self.conv1(x)  # 2.利用一次通道数为1的卷积调整通道数\n",
    "\n",
    "        return self.sigmoid(x)  # 3.取一个sigmoid获得输入特征层每一个特征点的权值\n",
    "\n",
    "\n",
    "\n",
    "# CBAM\n",
    "class cbam_block(nn.Module):\n",
    "    def __init__(self, channel, ratio=8, kernel_size=7):\n",
    "        super(cbam_block, self).__init__()\n",
    "        self.channelattention = ChannelAttention(channel, ratio=ratio)\n",
    "        self.spatialattention = SpatialAttention(kernel_size=kernel_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x * self.channelattention(x)\n",
    "        x = x * self.spatialattention(x)\n",
    "        return x\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-marble",
   "metadata": {},
   "source": [
    "### 3.2卷积神经网络（主干网络）\n",
    "\n",
    "#### ·可学习layer\n",
    "\n",
    ">**卷积层**\n",
    ">>nn.Conv2d(\n",
    ">>&nbsp;&nbsp;&nbsp;  in_channels = 输入特征矩阵的深度,   \n",
    ">>&nbsp;&nbsp;&nbsp;  out_channels = 卷积核数|输出特征矩阵深度,   \n",
    ">>&nbsp;&nbsp;&nbsp;  kernel_size = (卷积核长, 卷积核宽),   \n",
    ">>&nbsp;&nbsp;&nbsp;  stride = 卷积框步长,   \n",
    ">>&nbsp;&nbsp;&nbsp;  padding = (填充上下行数, 填充左右列数),   \n",
    ">>&nbsp;&nbsp;&nbsp;  dilation = 卷积核元素之间的间距1,   \n",
    ">>&nbsp;&nbsp;&nbsp;  groups = 从输入通道到输出通道的阻塞连接数1,   \n",
    ">>&nbsp;&nbsp;&nbsp;  bias = 添加偏置T/F,   \n",
    ">>&nbsp;&nbsp;&nbsp;  padding_mode = '填充数字zeros'  \n",
    ">>)\n",
    "\n",
    ">>说明：\n",
    ">>>in_channels = 输入通道维的元素数  \n",
    ">>>图像(通道，图像长，图像宽)（黑白图像通道=1，RGB图像通道=3）   \n",
    ">>>out_channels = 提取特征数 = 输出特征矩阵深度 = 输出特征矩阵通道维\n",
    "\n",
    ">**全连接层**\n",
    ">>nn.Linear(in_features=每个输入样本的大小, out_features=每个输出样本的大小)  \n",
    ">>说明：\n",
    "\n",
    "\n",
    "#### ·不可学习layer\n",
    ">**将多维数据转成一维**\n",
    ">>x.view(x.size(0), -1)  \n",
    ">>说明：在卷积层转全连接层之间使用，用在forward(self, x)中"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAAxCAYAAAAcAd90AAAFG0lEQVR4nO2bS27bMBCGmT4WOodRoECv0It544XP1itk6TN0KRRZpJvKYJl5k5Jszf8BRmRLmiEZ/hxyJL7M8/xeAACH59PeBQAAbAPE3sF0ve5dBDc9Zbbeu5aPZ2zvRwJiH8R0vX74WK7v9em5jvtrtTFfLs4SFld7gHVJL/a6k3nE0wpmvlzuYqiPObzC6RHDfLncxbr4jYo36n8pA0X7O9W+oJ8vexdgT+oO7+n8tXiWjll35rqDjhZUa1vypZWrHqgkP9xvlL/Wt5W6TSm2HJyOSkqxR6I5Z6ceLEpZR9x1ZKztawLQoql2r3QtZbcto2cw3WKgzE5KsUudMmKnPrbaqqOu53ruu1ZOyp4FTug9Nrny9dQR6KQUexRqGhyNYpElQ1sGLQpK6961ROQZwBaoNTtEP56UYu9Zqy/31LaoY4vvCD05Bi/RqO6ZObUzHAh7PdKJnYp2ng7WXstFdsu9EX9WO73LFG+blKLPFjwRG6IfT8pHb97HZBTeKTz3e08STYLKJ7S2OTx1iA5GWln2eER4dNJF9pFYp/DeSKz5ar9bxGZNAnL2OKLJO2qGBGGvyws2wtjxTo17HsdFO762nIhMm60Jx9FihfjHArEDkISUa3YAMgKxA5AEiB2AJEDsADwQnteYvahiH7VpBKzD8j6+9ujvyP+7o9Rt7bciRbGP2CwC1kV7KajdNTcCbWAZjWbzCP1yC32RL9Ugmj8X1jfeRnSmrV9xzRBktqojKfZRW0CfEWpDx3Jsva/G8+Yad5+2u02yF3mxh/In2bT402x6/HH2evxZ0N5ajJyz+Ol5Oes/5nl+pz7lfCaPM3yW+m5Vb8mP5f/g/d1alnI+f7BhLau1Dj3+uPM9/iL/B82H5k/6LdLe3IedxmeM6jWRXV8t3qjBRYRRm1YstGv73l1qUh20d/itr+laymHxJ/mp/3psSu0ZKa/lHMcHsVMVyyp4K71tIw2sW7a7ND3v4Qh9J7pPob537/wXmY0fsQX0WVkjez2KEeUZldmOZuSjdYjct1aQ8tSvZzMT1wej9cIW13/sMQpbfEhT6vqYS2xx90TLJCWLrMm51odFPJa6c/d4/GnlaMviOWf1IfXBngEMu94A2BmrgHtnKhA7SMuIxOozAbEDkARshAEgCRA7AEmA2AFIAsQOwAMwTdPQ6yjwnP2BOZ1O9+Pb7SZew51/dqZpKvM87+q/lLJqGTx1nOc53CaI7A/M7XYTRXw6ne7X1ANDD1Lk6IkqUZt7Cv0R/I8EYj8IR43sRycSpZfo7iXVNL5uoLrB5Mb+Wn79+Fx+k+fey/fXP+WbwXcbeWtxWqbrnK3INL5th/a3tl2kcxabVFtrNil7Pf4stAKylkdrT+4+rQzRenCkEju13tEb8a38fH3r8rtMt+vv0jlNuMt5y7Utbf2X73UnbdtEOqfZnKbpv/MWf63PEf4i7UKVp71WKwv1VyLWR22kEnspkdGyP7LXa2pqHT5qvW1BElEPkr2t170j/HkSZqWMbc/REX0hndj99Ef2UuRovPV6u41MIzhSIsvLGu25BukSdO00awvaNbkUyUdE+WUqy52zCjOamY+26yMJxfPcu2ego/qh1kejPlNthKESOMv3NZGSc+15bm3fnpdsalNALXkk3R9J0C2/cW2uJeikeyL+OLTycba0JUy9Zuful5KMVHkgdgCelGgi0QPEDlLARd9MuQYk6EAKMomaI12CDoCsQOwAJOEv+dpqN4TReOMAAAAASUVORK5CYII="
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "b9608747",
   "metadata": {},
   "source": [
    "说明：在最后一个卷积层前加入注意力模块\n",
    "\n",
    "def __init__(self):\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "def forward(self, x):\n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0408f733",
   "metadata": {},
   "source": [
    "#### （1）加入注意力机制1 eca注意力模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "binding-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在第二个卷积层后插入eca注意力模块\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 定义神经网络类\n",
    "class CNN_eca(nn.Module):\n",
    "    # 可学习参数的层（如全连接层、卷积层等）\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 第一部分卷积层1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # 卷积层(输入通道维1，输出通道维16，卷积窗口3*3)\n",
    "            nn.Conv2d(1, 16, kernel_size=(3,3), stride=1, padding=1),  # 维度变换(1,28,28) （黑白图像1通道，长28像素，宽28像素）->(16,28,28) （16个卷积核提取16个特征通道，长28，宽28）图像边缘扩展，没被卷积抛去\n",
    "            # 激活函数\n",
    "            nn.ReLU(),\n",
    "            # 池化层\n",
    "            nn.MaxPool2d(2) # 维度变化(16,28,28)->(16,14,14)\n",
    "        )\n",
    "            \n",
    "        # 第二部分卷积层2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # 卷积层(输入通道维16，输出通道维32，卷积窗口3*3)\n",
    "            nn.Conv2d(16, 32, kernel_size=(3,3), stride=1, padding=1),  # 维度变换(16,14,14)->(32,14,14)\n",
    "            # 激活函数\n",
    "            nn.ReLU(),\n",
    "            # 池化层\n",
    "            nn.MaxPool2d(2) # 维度变化(32,14,14)->(32,7,7)\n",
    "        )\n",
    "        \n",
    "        # 注意力机制eca模块\n",
    "        self.attention_eca = eca_block(32)\n",
    "#        self.attention_cbam = cbam_block(32)\n",
    "        \n",
    "            \n",
    "        # 全连接层\n",
    "        self.out = nn.Linear(32*7*7, 10)\n",
    "            \n",
    "            \n",
    "        \n",
    "    # 实现模型的功能，实现各个层之间的连接关系\n",
    "    # nn.functional实现不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)的构造\n",
    "    def forward(self, x):\n",
    "        # 执行卷积层1 conv1\n",
    "        x = self.conv1(x)\n",
    "        # 执行卷积层2 conv2\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # 插入注意力机制\n",
    "        x = self.attention_eca(x)\n",
    "#        x = self.attention_cbam(x)\n",
    "        \n",
    "        # 将图像数据转为1维\n",
    "        x = x.view(x.size(0),-1)\n",
    "        # 执行全连接层 out\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aea6420",
   "metadata": {},
   "source": [
    "#### （2）加入注意力机制1 CBAM注意力模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d908899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 在第二个卷积层后插入cbam注意力模块\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 定义神经网络类\n",
    "class CNN_cbam(nn.Module):\n",
    "    # 可学习参数的层（如全连接层、卷积层等）\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 第一部分卷积层1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # 卷积层(输入通道维1，输出通道维16，卷积窗口3*3)\n",
    "            nn.Conv2d(1, 16, kernel_size=(3,3), stride=1, padding=1),  # 维度变换(1,28,28) （黑白图像1通道，长28像素，宽28像素）->(16,28,28) （16个卷积核提取16个特征通道，长28，宽28）图像边缘扩展，没被卷积抛去\n",
    "            # 激活函数\n",
    "            nn.ReLU(),\n",
    "            # 池化层\n",
    "            nn.MaxPool2d(2) # 维度变化(16,28,28)->(16,14,14)\n",
    "        )\n",
    "            \n",
    "        # 第二部分卷积层2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # 卷积层(输入通道维16，输出通道维32，卷积窗口3*3)\n",
    "            nn.Conv2d(16, 32, kernel_size=(3,3), stride=1, padding=1),  # 维度变换(16,14,14)->(32,14,14)\n",
    "            # 激活函数\n",
    "            nn.ReLU(),\n",
    "            # 池化层\n",
    "            nn.MaxPool2d(2) # 维度变化(32,14,14)->(32,7,7)\n",
    "        )\n",
    "        \n",
    "        # 注意力机制eca模块\n",
    "#        self.attention_eca = eca_block(32)\n",
    "        self.attention_cbam = cbam_block(32)\n",
    "        \n",
    "            \n",
    "        # 全连接层\n",
    "        self.out = nn.Linear(32*7*7, 10)\n",
    "            \n",
    "            \n",
    "        \n",
    "    # 实现模型的功能，实现各个层之间的连接关系\n",
    "    # nn.functional实现不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)的构造\n",
    "    def forward(self, x):\n",
    "        # 执行卷积层1 conv1\n",
    "        x = self.conv1(x)\n",
    "        # 执行卷积层2 conv2\n",
    "        x = self.conv2(x)\n",
    "        \n",
    "        # 插入注意力机制\n",
    "#        x = self.attention_eca(x)\n",
    "        x = self.attention_cbam(x)\n",
    "        \n",
    "        # 将图像数据转为1维\n",
    "        x = x.view(x.size(0),-1)\n",
    "        # 执行全连接层 out\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa53c7",
   "metadata": {},
   "source": [
    "# 4.训练模型\n",
    "\n",
    "### 手动训练  （加入注意力机制不改变训练方法，此处仅为了保存结果）\n",
    "\n",
    "1.实例化模型\n",
    "```\n",
    "model变量 = 自定义神经网络类名()\n",
    "```\n",
    "\n",
    "2.定义损失函数和优化器\n",
    "```\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model变量.parameters(), lr = 学习率) \n",
    "```\n",
    "\n",
    "3.训练模型\n",
    "```\n",
    "for epoch in range(训练轮数):  # 训练轮数\n",
    "    \n",
    "    # 遍历训练集每条数据，进行训练 \n",
    "    running_loss = 0.0 # 记录一轮中每条训练数据预测的损失累加\n",
    "    for step, (x, y) in enumerate(训练集加载train_loader):   #【enumerate()枚举对象 得到格式（id，元素）】\n",
    "        b_x = Variable(x) # 数据x\n",
    "        b_y = Variable(y) # 标签y\n",
    "    \n",
    "        output = model_2(b_x) # 把数据输入进网络\n",
    "        loss = loss_func(output, b_y) # 计算一条数据的损失\n",
    "        running_loss += loss.item() # 损失累加\n",
    "        \n",
    "        optimizer.zero_grad() # 梯度置零\n",
    "        loss.backward()  # loss反向传播\n",
    "        optimizer.step() # 反向传播后参数更新\n",
    "        \n",
    "    \n",
    "    print('训练轮数：', epoch, ' 平均损失：',running_loss/len(train_loader)) #平均损失=每轮每条训练数据损失求和/每轮训练数据数\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a3c953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "# 自定义训练方法\n",
    "def train(model):\n",
    "    \n",
    "    EPOCH = 3 # 训练轮数\n",
    "\n",
    "    #记录用于绘图\n",
    "    losses = []#记录每次迭代后训练的loss\n",
    "    eval_losses = []#测试的\n",
    "    \n",
    "    \n",
    "    # 损失函数\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    # 优化器\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = 0.02) \n",
    "    \n",
    "\n",
    "\n",
    "    for epoch in range(EPOCH):  # 训练轮数\n",
    "        # 遍历训练集每条数据，进行训练，得到每轮损失loss\n",
    "        running_loss = 0.0\n",
    "        for step, (x, y) in enumerate(train_loader):   #【enumerate()枚举对象 得到格式（id，元素）】\n",
    "            b_x = Variable(x) # 数据x\n",
    "            b_y = Variable(y) # 标签y\n",
    "\n",
    "            output = model(b_x) # 把数据输入进网络\n",
    "            loss = loss_func(output, b_y) # 计算一条数据的损失\n",
    "            running_loss += loss.item() # 损失累加\n",
    "\n",
    "            optimizer.zero_grad() # 梯度置零\n",
    "            loss.backward()  # loss反向传播\n",
    "            optimizer.step() # 反向传播后参数更新\n",
    "\n",
    "        losses.append(running_loss/len(train_loader)) # 记录该轮平均损失，后续用于画图\n",
    "        print('训练轮数：', epoch, ' 训练平均损失loss：',running_loss/len(train_loader)) #平均损失=每轮每条训练数据损失求和/每轮训练数据数\n",
    "\n",
    "\n",
    "\n",
    "        # 遍历测试集每条数据，进行测试，每轮训练后损失val_loss\n",
    "        running_loss = 0.0\n",
    "        for step, (x, y) in enumerate(test_loader): \n",
    "            b_x = Variable(x) # 数据x\n",
    "            b_y = Variable(y) # 标签y\n",
    "\n",
    "            output = model(b_x) # 把数据输入进网络\n",
    "            loss = loss_func(output, b_y) # 计算一条数据的损失\n",
    "            running_loss += loss.item() # 损失累加\n",
    "\n",
    "        eval_losses.append(running_loss/len(test_loader)) # 记录该轮平均损失，后续用于画图\n",
    "        print('训练轮数：', epoch, ' 测试平均损失val_loss：',running_loss/len(test_loader)) #平均损失=每轮每条训练数据损失求和/每轮训练数据数\n",
    "\n",
    "        \n",
    "    print('end') \n",
    "    return losses, eval_losses\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460e1ba",
   "metadata": {},
   "source": [
    "#### （1）引入ecan注意力模块的cnn网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "83c03e60",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'CNN_eca' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-2a2423e97739>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 初始化模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCNN_eca\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# 查看模型\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'CNN_eca' is not defined"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "model = CNN_eca()\n",
    "\n",
    "# 查看模型\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ea45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "\n",
    "losses, eval_losses = train(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(losses)), losses, marker='o',label='losses')\n",
    "plt.plot(range(len(eval_losses)), eval_losses, marker='*',label='eval_losses')\n",
    "plt.legend() #图例\n",
    "plt.title(\"loss\") #标题\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d33dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型 .save('模型.h5')\n",
    "\n",
    "torch.save(model,'pytorch_number_ecan.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4d54f8",
   "metadata": {},
   "source": [
    "#### （2）引入CBAM注意力模块的cnn网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3d92ec4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_cbam(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (attention_cbam): cbam_block(\n",
      "    (channelattention): ChannelAttention(\n",
      "      (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "      (max_pool): AdaptiveMaxPool2d(output_size=1)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=32, out_features=4, bias=False)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=4, out_features=32, bias=False)\n",
      "      )\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "    (spatialattention): SpatialAttention(\n",
      "      (conv1): Conv2d(2, 1, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), bias=False)\n",
      "      (sigmoid): Sigmoid()\n",
      "    )\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "model = CNN_cbam()\n",
    "\n",
    "# 查看模型\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "90102e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练轮数： 0  训练平均损失loss： 0.20297105824733688\n",
      "训练轮数： 0  测试平均损失val_loss： 0.09809135964384214\n",
      "训练轮数： 1  训练平均损失loss： 0.0869029611423572\n",
      "训练轮数： 1  测试平均损失val_loss： 0.07421995443499847\n",
      "训练轮数： 2  训练平均损失loss： 0.07839510338179874\n",
      "训练轮数： 2  测试平均损失val_loss： 0.09121088100142688\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "# 训练\n",
    "\n",
    "losses, eval_losses = train(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1afbe63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAya0lEQVR4nO3deXwU9fnA8c+T+wJyciXkQBAIV5ANCLTU1guPVrylinIoWo+irVrb2tZaf7VqW60WpSqI1gtUtNajWK96gJgA4QblCCHhyEWA3Nf398csZBMSsoFsZnfzvF+vfZGd+c7sk2XyzMx3nvmOGGNQSinlvwLsDkAppZRnaaJXSik/p4leKaX8nCZ6pZTyc5rolVLKz2miV0opP6eJXnV7IpIrImfZHYdSnqKJXiml/JwmeqWU8nOa6JVyEpFQEXlMRPY4X4+JSKhzXryIvCMiZSJSKiKfi0iAc94vRKRARA6LyFYROdPe30Sp5oLsDkApL/Jr4HQgAzDAv4B7gd8APwfygQRn29MBIyJDgFuBTGPMHhFJBQK7Nmyljk+P6JVqcjVwvzGm0BhTBPwemO6cVwf0A1KMMXXGmM+NNVBUAxAKpItIsDEm1xiz3ZbolWqDJnqlmvQHdrm83+WcBvAIsA34QER2iMg9AMaYbcDtwH1AoYi8KiL9UcqLaKJXqskeIMXlfbJzGsaYw8aYnxtjBgI/BH52pC/eGPOyMeY7zmUN8FDXhq3U8WmiV6rJK8C9IpIgIvHAb4EXAUTkQhEZJCICHMLqsmkQkSEi8gPnRdtqoMo5TymvoYleqSYPANnAOmA9sNo5DWAw8CFQDqwAnjTGfIrVP/8noBjYB/QGftWlUSvVDtEHjyillH/TI3qllPJzmuiVUsrPaaJXSik/p4leKaX8nFcOgRAfH29SU1PtDkMppXzGqlWrio0xCa3N88pEn5qaSnZ2tt1hKKWUzxCRXW3N064bpZTyc5rolVLKz2miV0opP+eVffRKKd9SV1dHfn4+1dXVdofi98LCwkhKSiI4ONjtZTTRK6VOWn5+Pj169CA1NRVr3DflCcYYSkpKyM/PJy0tze3l/CbRv7WmgEeWbWVPWRX9o8O569whTB2TaHdYSnUL1dXVmuS7gIgQFxdHUVFRh5bzi0T/1poCfrl0PVV11uiwBWVV/HLpegBN9kp1EU3yXeNEvme/uBj7yLKtR5P8EVV1DTyybKtNESmllPfwi0S/p6yqQ9OVUv4nKirK7hC8ll903fSPDqeglaTePzrchmiUUu3Ra2pdy60jehGZIiJbRWTbkYcit5h/tYisc76Wi8hod5ftDHedO4Tw4MBm0wIEfn72qZ74OKXUSThyTa2grApD0zW1t9YUdMr6jTHcddddjBgxgpEjR7J48WIA9u7dy+TJk8nIyGDEiBF8/vnnNDQ0MGPGjKNtH330UQC2b9/OlClTGDt2LN/97nfZsmULAK+99hojRoxg9OjRTJ48uVPi7QrtHtGLSCAwDzgbyAeyRORtY8wml2Y7ge8ZYw6IyHnA08B4N5c9aUeOBI4cIfQMD+ZgVR0b9hzikrGd+UlKqfb8/t8b2bTnUJvz1+SVUdvQ2GxaVV0Dd7++jle+zmt1mfT+PfndD4e79flLly4lJyeHtWvXUlxcTGZmJpMnT+bll1/m3HPP5de//jUNDQ1UVlaSk5NDQUEBGzZsAKCsrAyAOXPmMH/+fAYPHszKlSu5+eab+fjjj7n//vtZtmwZiYmJR9v6Ane6bsYB24wxOwBE5FXgIuBosjbGLHdp/xWQ5O6ynWXqmMRmp36///dGFn65k7T4CKZPSO3sj1NKnaCWSb696R31xRdfMG3aNAIDA+nTpw/f+973yMrKIjMzk1mzZlFXV8fUqVPJyMhg4MCB7Nixg9tuu40LLriAc845h/LycpYvX87ll19+dJ01NTUATJo0iRkzZnDFFVdwySWXdEq8XcGdRJ8I7HZ5nw+MP0772cD7HV1WROYAcwCSk5PdCOv47r0gnbySSu779yYGxEZwxpDeJ71OpVT72jvynvSnj1u9ppYYHc7iGyec9Oe39RzsyZMn89lnn/Huu+8yffp07rrrLq699lrWrl3LsmXLmDdvHkuWLOGxxx4jOjqanJycY9Yxf/58Vq5cybvvvktGRgY5OTnExcWddMye5k4ffWtFm61+kyLyfaxE/4uOLmuMedoY4zDGOBISWh1SuUMCA4THp41hSJ8e3PryGrbsa/tUUinVdVq7phYeHMhd5w7plPVPnjyZxYsX09DQQFFREZ999hnjxo1j165d9O7dmxtuuIHZs2ezevVqiouLaWxs5NJLL+UPf/gDq1evpmfPnqSlpfHaa68B1o5j7dq1gNV3P378eO6//37i4+PZvXv38ULxGu4k+nxggMv7JGBPy0YiMgp4FrjIGFPSkWU9JTI0iAUzHESGBjJ7UTaFh3UcDqXsNnVMIg9eMpLE6HAE60j+wUtGdlrVzcUXX8yoUaMYPXo0P/jBD3j44Yfp27cvn376KRkZGYwZM4Y33niDuXPnUlBQwBlnnEFGRgYzZszgwQcfBOCll15iwYIFjB49muHDh/Ovf/0LgLvuuouRI0cyYsQIJk+ezOjRo48XiteQtk5zjjYQCQK+Ac4ECoAs4MfGmI0ubZKBj4FrXfvr3Vm2NQ6Hw3Tmg0c2FBzk8vkrOLVPFK/OmUB4SGD7Cyml3LZ582aGDRtmdxjdRmvft4isMsY4Wmvf7hG9MaYeuBVYBmwGlhhjNorITSJyk7PZb4E44EkRyRGR7OMte2K/2okbkdiLx6eNYV3BQe5YnENj4/F3bkop5U/cumHKGPMe8F6LafNdfr4euN7dZe1wdnof7r0gnT+8s4mHlm3hl+fp0YdSqnvwiztj3TVrUio7i8v5x/92kBoXybRxJ1/do5RS3q5bJXoR4b4fDmd3aRW/eWsDA2Ii+M7geLvDUkopj/KLQc06IigwgL//eAynJETxk5dW8e3+w3aHpJRSHtXtEj1Aj7BgFsxwEBoUyKznsygur7E7JKWU8phumegBkmIiWHCdg6LDNcx5IZvqFuPZK6WUv+i2iR5g9IBoHrsyg9V5Zdz52lotu1SqKx3eB8+dB4f32x3JUampqRQXF7c531fHvO/WiR5gyoh+3HPeUN5Zt5dHP/zG7nCU6j7+9zDkfQX/e8juSPxet6q6acuNkweSW1zBEx9vIyUuksvGJrW/kFKqde/fA/vWtz0/70twvSM/e4H1EoHkSa0v03cknPen437siy++yOOPP05tbS3jx49n1KhR7Nq1i4cffhiARYsWsWrVKp544gmmTp3K7t27qa6uZu7cucyZM6dDv6Ixhrvvvpv3338fEeHee+/lyiuvZO/evVx55ZUcOnSI+vp6nnrqKSZOnMjs2bPJzs5GRJg1axZ33HEH27dv55ZbbqGoqIiIiAieeeYZhg4dymuvvcbvf/97AgMD6dWrF5999lmHYmuNJnqssss/TB3B7gOV/HLpOhKjw5lwivePSKeUT+qfCQd2QlUJmEaQAIiIg5i0E17l5s2bWbx4MV9++SXBwcHcfPPNREVFsXTp0qOJfvHixfz6178GYOHChcTGxlJVVUVmZiaXXnpph0ah9LUx7zXROwUHBvDk1WO59Knl3PTiKt68eSIDE3yzP04pW7Vz5A3Av++A1YsgKAwaamHYj+DCv57wR3700UesWrWKzMxMAKqqqujduzcDBw7kq6++YvDgwWzdupVJk6wzhscff5w333wTgN27d/Ptt992KNH72pj33b6P3lWv8GCem5FJUIAwa1EWpRW1doeklH+qKISxM+H6D61/y0/ugqwxhuuuu46cnBxycnLYunUr9913H1deeSVLlizhjTfe4OKLL0ZE+PTTT/nwww9ZsWIFa9euZcyYMVRXd2xk2/bGvE9MTGT69Om88MILxMTEsHbtWs444wzmzZvH9ddfT2Nj49Ex74+8Nm/eDFhj3j/wwAPs3r2bjIwMSkpKWv2sDgfsba+xY8caO2XnlprBv37PXP7UclNdV29rLEr5gk2bNtn6+Rs3bjSDBg0y+/fvN8YYU1JSYnJzc01paalJS0szZ5xxhlm5cqUxxpi33nrLXHjhhcYYYzZv3mxCQ0PNJ598YowxJiUlxRQVFbX5OZGRkcYYY9544w1zzjnnmPr6elNYWGiSk5PN3r17TW5urqmrqzPGGPPoo4+auXPnmqKiInPw4EFjjDFr1qwxo0ePNsYYM2HCBLNkyRJjjDGNjY0mJyfHGGPMtm3bjn5eRkaGWbNmzTFxtPZ9A9mmjZyqR/StGJsSw18uH83XuaXc88b6NvfeSinvkJ6ezgMPPMA555zDqFGjOPvss9m7dy8xMTGkp6eza9cuxo0bB8CUKVOor69n1KhR/OY3v+H000/v8Of52pj37Y5Hb4fOHo/+RP3942/58wffcMdZpzL3rMF2h6OU19Lx6LtWR8ej14uxx3HL9wexs7iSRz/8htT4CC7K6Jwn4CilVFfSRH8cIsKDl4wk/0Ald71mlV06UmPtDksp5UElJSWceeaZx0z/6KOPfOJB4K3RRN+OkKAA5l8zlkueWs6cf1pllylxkXaHpZTXMcYgInaHcdLi4uLIycmxO4w2nUh3u16MdUNMZAgLZ2TSaAwzF2VxsLLO7pCU8iphYWGUlJRo4YKHGWMoKSkhLCysQ8vpxdgO+HpnKVc/+xWOlFienzWOkCDdTyoFUFdXR35+fofr0VXHhYWFkZSURHBwcLPpejG2k4xLi+Xhy0Zxx+K13PvWeh66dJRfnKoqdbKCg4NJSzvxIQyUZ2mi76CLxySxs7iSxz/6ltT4SG4+Y5DdISml1HG51fcgIlNEZKuIbBORe1qZP1REVohIjYjc2WLeHSKyUUQ2iMgrItKxziUvdMdZg/nR6P48/J+tvLtur93hKKXUcbWb6EUkEJgHnAekA9NEJL1Fs1Lgp8CfWyyb6JzuMMaMAAKBqzohbluJCA9fNgpHSgw/W5LDmrwDdoeklFJtcueIfhywzRizwxhTC7wKXOTawBhTaIzJAlorRwkCwkUkCIgA9pxkzF4hLDiQf0wfS5+eYdzwQja7SyvtDkkppVrlTqJPBHa7vM93TmuXMaYA6yg/D9gLHDTGfNDRIL1VXFQoC2dkUlvfyKxFWRyq1rJLpZT3cSfRt1ZW4lZNpojEYB39pwH9gUgRuaaNtnNEJFtEsouKitxZvVcY1DuK+deMZWdxBbe8tJq6hka7Q1JKqWbcSfT5wACX90m43/1yFrDTGFNkjKkDlgITW2tojHnaGOMwxjgSEhLcXL13mDgonj9eMpLPvy3md29v1JtGlFJexZ1EnwUMFpE0EQnBupj6tpvrzwNOF5EIsQrOzwQ2n1io3u0KxwB+csYpvLwyjwVf7LQ7HKWUOqrdOnpjTL2I3Aosw6qaWWiM2SgiNznnzxeRvkA20BNoFJHbgXRjzEoReR1YDdQDa4CnPfOr2O+uc4aQV1LJ/723mQGxEZw7vK/dISmllA6B0Nmq6xq46umv2LrvMEtunMDIpF52h6SU6gaONwSCDtbSycKCA3nmWgexkSHMfj6LPWVVdoeklOrmNNF7QEKPUJ6bmUlVbQOzFmVRXlNvd0hKqW5ME72HnNqnB/OuPo1vC8u57eXV1GvZpVLKJproPWjyqQncf9FwPtlaxAPv+mWxkVLKB+jolR529fgUcosreObznaTGRTBjkg7lqpTqWprou8A95w0jt6SS+9/ZRHJcBD8Y2sfukJRS3Yh23XSBwADhb1dlMLx/L257eQ2b9hyyOySlVDeiib6LRIQE8ex1DnqGBzP7+Sz2H9JHrimluoYm+i7Up2cYC67L5FBVHbOfz6KyVssulVKep4m+i6X378nff3wam/YcYu6rOTQ0et+dyUop/6KJ3gbfH9qb3/1wOP/dtJ8H39OyS6WUZ2nVjU2um5jKzuIKnv1iJ6nxkVxzeordISml/JQmehv95sJ08kor+d3bGxkQG8H3TvWtcfiVUr5Bu25sFBggPD5tDKf26cGtL61m677DdoeklPJDmuhtFhUaxMIZDsJDApm1KIuiwzV2h6SU8jOa6L1Av17hLLguk9KKWq5/IZuq2ga7Q1JK+RFN9F5iZFIv/nZVBuvyy/j5azk0atmlUqqTaKL3IucM78uvzx/Ge+v38cgHW+0ORynlJ7TqxsvM/k4aO4sreOrT7aTFRXJF5gC7Q1JK+ThN9F5GRLjvR8PJK63kV2+uJykmnImD4u0OSynlw7TrxgsFBwYw7+rTGJgQyU0vrmJbYbndISmlfJgmei/VMyyYBddlEhIUwKxFWZSUa9mlUurEuJXoRWSKiGwVkW0ick8r84eKyAoRqRGRO1vMixaR10Vki4hsFpEJnRW8vxsQG8Ez1zrYf6iaOf9cRXWdll0qpTqu3UQvIoHAPOA8IB2YJiLpLZqVAj8F/tzKKv4G/McYMxQYDegoXh0wJjmGR6/MYNWuA9z9+jqM0bJLpVTHuHNEPw7YZozZYYypBV4FLnJtYIwpNMZkAXWu00WkJzAZWOBsV2uMKeuMwLuT80f24+4pQ3h77R4e/fBbu8NRSvkYdxJ9IrDb5X2+c5o7BgJFwHMiskZEnhWRyNYaisgcEckWkeyioiI3V999/OR7p3ClYwCPf/QtS1fn2x2OUsqHuJPopZVp7vYfBAGnAU8ZY8YAFcAxffwAxpinjTEOY4wjIUFHcWxJRPjD1BFMPCWOX7yxjpU7SuwOSSnlI9xJ9PmA6107ScAeN9efD+QbY1Y637+OlfjVCQgJCuCpq8eSHBvBjS+uYmdxhd0hKaV8gDuJPgsYLCJpIhICXAW87c7KjTH7gN0iMsQ56Uxg0wlFqgDoFRHMwhmZBIgwa1EWBypq7Q5JKeXl2k30xph64FZgGVbFzBJjzEYRuUlEbgIQkb4ikg/8DLhXRPKdF2IBbgNeEpF1QAbwRw/8Ht1KSlwkT08fS8GBKm58cRW19Y12h6SU8mLijeV6DofDZGdn2x2G1/tXTgFzX83hktMS+cvloxFp7XKKUqo7EJFVxhhHa/N0rBsfdlFGIrtKKvnrf78hLS6S284cbHdISikvpInex932g0HkFlfwl/9+Q0p8JD8a3d/ukJRSXkYTvY8TER68dCT5ZVXc+dpaEqPDGJsSa3dYSikvooOa+YHQoED+cc1Y+vcK44YXVpFXUml3SEopL6KJ3k/ERIawcEYmjcYwc9HXHKyqa38hpVS3oInejwxMiGL+NWPJK63k5pdWUdegZZdKKU30fuf0gXH86ZJRfLmthHvf3KCjXSql9GKsP7p0bBK5JRU88fE20hIiuel7p9gdklLKRpro/dTPzj6V3JJK/vT+FlJiIzhvZD+7Q1JK2US7bvyUiPDIZaM4LTma2xfnkLO7zO6QlFI20UTvx8KCA3nmWge9e4Zy/fPZ5B/QskuluiNN9H4uLiqU52ZkUlPfwOxF2Ryq1rJLpbobTfTdwKDePZh/zVi2F5Vz68trqNeyS6W6FU303cSkQfE8MHUEn31TxH3/3qhll0p1I1p1041cNS6ZnSUV/ON/O0iLj2L2d9LsDkkp1QU00Xczvzh3KHkllTzw7iaSYyM4O72P3SEppTxMu266mYAA4a9XZDAqsRc/fWUNGwoO2h2SUsrDNNF3Q+EhgTxznYPYyBBmP5/F3oNVdoeklPIgTfTdVO8eYSyckUlFjVV2WVFTb3dISikP0UTfjQ3p24N5V5/G1v2H+ekra2ho1EocpfyRJvpu7nunJnDfj4bz0ZZCHnh3k93hKKU8QKtuFNNPTyG3uIIFX+wkLT6Sayek2h2SUqoTuXVELyJTRGSriGwTkXtamT9URFaISI2I3NnK/EARWSMi73RG0Krz/er8YZw1rA/3vb2RT7YU2h2OUqoTtZvoRSQQmAecB6QD00QkvUWzUuCnwJ/bWM1cYPNJxKk8LDBA+NtVGQzr15NbX17N5r2H7A5JKdVJ3DmiHwdsM8bsMMbUAq8CF7k2MMYUGmOygGNGzBKRJOAC4NlOiFd5UGRoEAuuy6RHWDCzF2VReKja7pCUUp3AnUSfCOx2eZ/vnOaux4C7geOOpCUic0QkW0Syi4qKOrB61Zn69gpjwQwHZVV1zH4+m8paLbtUyte5k+illWlu1eGJyIVAoTFmVXttjTFPG2McxhhHQkKCO6tXHjK8fy+emDaGjXsOcsfiHBq17FIpn+ZOos8HBri8TwL2uLn+ScCPRCQXq8vnByLyYociVLY4c1gffnNhOss27ueh/2yxOxyl1ElwJ9FnAYNFJE1EQoCrgLfdWbkx5pfGmCRjTKpzuY+NMdeccLSqS82YmMq1E1L4x2c7eOXrPLvDUUqdoHbr6I0x9SJyK7AMCAQWGmM2ishNzvnzRaQvkA30BBpF5HYg3RijpRs+TET47YXp5JVWcu9bG0iKCee7g7VbTSlfI974AAqHw2Gys7PtDkM5ldfUc9lTyyk4UMXSmycyuE8Pu0NSSrUgIquMMY7W5ukQCKpdUaFBLJiRSVhIIDMXZVF0uMbukJRSHaCJXrklMTqcBdc5KC6v4YYXsqmua7A7JKWUmzTRK7eNSormsSvHsDa/jJ8vWatll0r5CE30qkOmjOjLL88byrvr9/KX/261OxyllBt09ErVYTd8dyA7iyuZ98l2UuMiudwxoP2FlFK20USvOkxEuP+i4eQfqORXb64nKSaCCafE2R2WUqoN2nWjTkhwYADzrj6N1LhIbnpxFduLyu0OSSnVBk306oT1DAtm4YxMggOFWYuyKK2otTskpVQrNNGrkzIgNoKnr3Ww72A1N/4zm5p6LbtUyttoolcn7bTkGP5yxWiycg9w9+vr8Ma7rZXqzvRirOoUF47qz66SSh5ZtpW0+EhuP+tUu0NSSjlpoled5uYzTmFncQWPffgtqXGRTB3TkefTKKU8RbtuVKcREf548UhOHxjL3a+vIyu31O6QlFJooledLCQogPnXjCUpJpw5L2STW1xhd0hKdXua6FWni44I4bmZmQDMWpRFWaWWXSplJ030yiNS4iJ5+loH+QequOnFVdTWH/fZ8EopD9JErzwmMzWWhy8bxVc7SvnVm+u17FIpm2jVjfKoqWMSyS2xKnHS4iO55fuD7A5JqW5HE73yuLlnDia3uIJHlm0lJS6CC0f1tzskpboV7bpRHiciPHTZKDJTY/jZkrWszjtgd0hKdSua6FWXCA0K5B/THfTrFcYNz2ezu7TS7pCU6jbcSvQiMkVEtorINhG5p5X5Q0VkhYjUiMidLtMHiMgnIrJZRDaKyNzODF75ltjIEBbOyKS+0TBrURYHq+rsDkmpbqHdRC8igcA84DwgHZgmIuktmpUCPwX+3GJ6PfBzY8ww4HTgllaWVd3IKQlRzL9mLLklFdzy0mrqGrTsUilPc+eIfhywzRizwxhTC7wKXOTawBhTaIzJAupaTN9rjFnt/PkwsBnQAVC6uQmnxPHHi0fyxbZifvuvDVp2qZSHuVN1kwjsdnmfD4zv6AeJSCowBljZ0WWV/7ncMYDckgrmfbKdtPhI5kw+xe6QlPJb7iR6aWVahw7BRCQKeAO43RhzqI02c4A5AMnJyR1ZvfJRPz97CLkllTz4/haSYyOZMqKv3SEp5Zfc6brJBwa4vE8C9rj7ASISjJXkXzLGLG2rnTHmaWOMwxjjSEhIcHf1yocFBAh/uXw0GQOiuX3xGtbll9kdklJ+yZ1EnwUMFpE0EQkBrgLedmflIiLAAmCzMeavJx6m8ldhwYE8c62D+KhQZj+fTUFZld0hKeV32k30xph64FZgGdbF1CXGmI0icpOI3AQgIn1FJB/4GXCviOSLSE9gEjAd+IGI5Dhf53vst1E+KT4qlOdmZFJd28DsRVkcrtayS6U6k3hjxYPD4TDZ2dl2h6G62OffFjHjuSy+OzieZ691EBSo9/Mp5S4RWWWMcbQ2T/+SlNf47uAEHpg6gk+3FnH/O5u07FKpTqKDmimvMm1cMrnFFfzjsx2kxUcyc1Ka3SEp5fM00Suv84spQ8ktqeAP72wiOTaCM4f1sTskpXyadt0orxMQIDx25RhGJPbitlfWsKHgoN0hKeXTNNErrxQeEsiz1zqIDg/m+uez2Xew2u6QlPJZmuiV1+rdM4wFMzIpr6ln9vNZVNTU2x2SUj5JE73yasP69eSJH49h895DzH01h4ZGrcRRqqM00Suv9/0hvbnvR8P5cPN+/vjeZrvDUcrnaNWN8gnXTkhlZ3EFC77YSWp8JNNPT7E7JKV8hiZ65TPuvSCdvJJK7nt7IwNiwjljSG+7Q1LKJ2jXjfIZgQHC49PGMKRPD259eQ1b9rU64rVSqgVN9MqnRIYGsWCGg8jQQGYvyqbwsJZdKtUeTfTK5/TrFc6C6zIprajlhuezqaptsDskpbyaJnrlk0Yk9uLxaWNYV3CQny3JoVHLLpVqkyZ65bPOTu/DvRek8/6GfTy8bKvd4SjltbTqRvm0WZNS2Vlczvz/bSc1LoKrxunzhpVqSRO98mkiwn0/HM7u0irufWsDA2IjmDQo3u6wlPIq2nWjfF5QYAB///EYTkmI4qYXV7Gt8LDdISnlVTTRK7/QIyyYBTMchAYFMnNRFsXlNXaHpJTX0ESv/EZSTAQLrnNQdLiGOS9kU12nZZdKgSZ65WdGD4jmsSszWJ1Xxp2vrdWyS6XQRK/80JQR/bjnvKG8s24vj374jd3hKGU7txK9iEwRka0isk1E7mll/lARWSEiNSJyZ0eWVcoTbpw8kKsyB/DEx9t4Y1W+3eEoZat2E72IBALzgPOAdGCaiKS3aFYK/BT48wksq1SnExH+MHUEkwbFcc/SdXy1o8TukJSyjTtH9OOAbcaYHcaYWuBV4CLXBsaYQmNMFlDX0WWV8pTgwACevHosKXGR3PjPVewoKrc7JKVs4U6iTwR2u7zPd05zh9vLisgcEckWkeyioiI3V9/C4X3w3HlweP+JLa/8Tq/wYJ6bkUlQgDBrURYHKmrtDkmpLudOopdWprlbyuD2ssaYp40xDmOMIyEhwc3Vt/C/hyHvK/jfQye2vPJLA2IjePpaB3sOVnPjP1dRU69ll6p7cSfR5wMDXN4nAXvcXP/JLOu+B3rDfb0gewGYRuvf+3pZ05UCxqbE8JfLR/N1bin3vLEeY7TsUnUf7iT6LGCwiKSJSAhwFfC2m+s/mWXdN3cdDL+k+bSQKBh6IeS8DAdyQf+wu70fju7PneecyptrCnji4212h6NUl2l3UDNjTL2I3AosAwKBhcaYjSJyk3P+fBHpC2QDPYFGEbkdSDfGHGpt2U7/LXr0hbBokAAICIKGOgiPhe0fw4Y3nG36Q8rEplf8EAjQ2wi6m1u+P4idxZX89b/fkBIXwUUZ7l5uUsp3iTeewjocDpOdnd2xhV69GqL6gGMmZD8H5fvhin9C0RbIWw67nK/De6324bGQPKEp8fcdBYE6mGd3UFvfyPQFK1mTV8bLN4zHkRprd0hKnTQRWWWMcbQ6z28SvTuMsbpxjiT9vOVQusOaFxIFA8ZZST95IiSOheCwzo9BeYUDFbVc8tRyDlbV8ebNE0mJi7Q7JKVOiib64zm013nEv8JK/oXOnqXAEEh0QIrzqH/AeAjt0TUxqS6xs7iCi5/8ktjIEN78ySR6RQTbHZJSJ0wTfUdUlsLulbDrSyvx78kB02D1//cdBSmTnEf9EyAyzp4YVaf5emcpVz/7FY6UWJ6fNY6QIL1uo3yTJvqTUVMO+VmQ5zziz8+C+mprXsLQpq6elInQSy/s+aI31+Rzx+K1XOFI4qFLRyHS2u0fSnm34yV6vfrYntAoOOX71gugvsY6yj9yxL/+dcheaM2LTmm6uJs8EeJOAU0aXu/iMUnsLK7k8Y++JTU+kpvPGGR3SEp1Kk30HRUUCsnjrdd3fwaNDbB/Q9MF3m//C2tfsdpG9m5e0tl7uJZ0eqk7zhpMbnEFD/9nK6lxkZw/sp/dIanu5vA+eH0mXLYIevTp1FVroj9ZAYHQb7T1Ov0nVmVP8bfNSzo3vWW1DesFA053Jv5J1jJBIbaGrywiwsOXjWJPWRV3LM6hX68wxiTH2B2W6k5ch3C58K+dumrto+8KZXnOqp4vrb7+YufDMILCYUCmlfSTJ0BSJoRE2BtrN1dSXsPFTy6nsraeN2+exIBY/f9QHtDYCGW7oHAzLJkOjfXHtgkKhXsL3V6lXoz1NuVFLiWdX1pdP6YRAoKh/xhnSeckq6QzPNruaLudbYXlXPLkl/TtFcbrP5lIzzAtu1QnyBjrJs3CTVZSL9xs/Vy0Feoqm9oFR1hFHqbROgAcdiGc838d6sLRRO/tqg/C7q+dF3hXQMEqaKwDBPqMaN7PH6UDtXWF5duKuXbh10w4JY6FMzIJDtRrK6odFSVNCb3IJalXH2xqE9UHeg+D3ulN/yYMgQ9+C6sXWffvNNTC2Jkd7r7RRO9r6qogP9tZ0vmltRM4svePG+QcusFZzx+drJU9HrIkezd3v76OH49P5v+mjtCyS2WpPmQdkR89Snf+W+HSzRIW7ZLMXRJ7RBvDbbQ2hMtVL3UoLE30vq6hDvauayrpzFsB1WXWvJ6JzUs6E4Zo4u9ED/1nC099up2pGf3Jyj3AnrIq+keHc9e5Q5g6Ru+b8Gt1Vdb1NNdkXrgZDro8Syk4EnoPtZJ4gktS79G3y/8ONdH7m8ZG69Rwl0tlT/k+a15EXPPB2vqM1MHaTkJjo+GSJ78kJ/9gs+nhwYE8eMlITfb+oKEOSrYf2+1SusPqMwerSyX+1GOP0Hsle03JtN4w5W8CAqDPcOs17gbrgk/pjqa7d3d9CVvesdqG9GgarC1lEiSeZl3NV24JCBAKD9ccM72qroGHl23RRO9LGhuhLBcKtzQ/Qi/+xnlNDGuok9hTrEQ+4rKmpB470KcPmPSI3l8d2tPUzbNrubVhAwSGQpKjabyeAeN0sLZ2pN3zbpvPzoyJCCYuKpS4yBDio0KJiwohLtL6Nz4q5Oi8uKhQeoYFaT9/V3C30iU6uenI/Ei3S/ypPjtqrR7Rd0c9+8PIy6wXWIO15X3V1M//+V+dg7U5b/g62s8/oe0LRt1U/+hwCsqqjpneIyyIC0f1p6SihuLyWrbsO0RJRS1llXWtric4UI7uBI7uAJw7gaM7hqM7iVDCggM9/av5PtdKl8JN1vMn2qp0GTujeaVLNzrA0SP67qqmHPK/dnb1rLAGa2twdlEkDGte0tmzv72x2uytNQX8cul6quqaHip+vD76uoZGDlTUUlxeS0lFDSXltRSX11BSUUtJufO98+fi8hqq6xpb/dzIkMCjO4G4yFDnGULzncGR9zERwQT5cwmoJypd/IxejFXtq6+BgtVNQzfkrYTaw9a8mNSmETpTJlr9ld2sC+KtNQU8smyrR6puKmvrm3YGzp1DcXnt0Z9ddxSlFbU0NB77NysCMRFHzhCss4R4l7OFph2F9b5HqJd2I/lYpYs30USvOq6hvmmwtiPJv7LEmhfVp+nibvIE64/MSyoP/F1jo+FgVd3RnUHp0TODpp2CdcZg/XywqvVupJDAAOcOocUZQrOdQ1MXU6d3I7WsdDnS7eJjlS7eRBO9OnnGWEdariWdh/KteWHRzpLOCU2DtQXqsAHeoLa+kQOVzc8WrDMEZzeSy46iuLyGmvrWu5GiQoOaJX/X6wktzx5iIkIIDHAeWXeo0qXFHaM+XunS1TTRK88oy2sq59y1Akq+taYHR1gDtB25ezfJAcHh9saq2mWMobK2odkZwZGdQWs7itKKGpp6kQx9OMCQgN0MCchnZPAehgTsJqVxN2GmqTy1PDyRyuhTaYgfSlDf4UQkDSei/zBEt4+TpoledY3ywua1/Ps2AMYarC3xtKahG5LHW0M2K9/krHRp3L+J2r0bMfs3EVy6laDaQ0ebHA6KIz84le0ygM0NSeTU9COnui8VHJvQQ4IC2riecOyF59jIEEKDtBqpNSed6EVkCvA3IBB41hjzpxbzxTn/fKASmGGMWe2cdwdwPWCA9cBMY0z18T5PE72fqCprGqwtb4V1sffIYG19R7g8f3ciRCXYHa1qqfqQs1xxc6dUutTUN3Cgoq6VCqTmZw8l5bUUlddQ20Y3Uo+wIOKjQomNbNmV5Fqqal1biHbtRvJzJ5XoRSQQ+AY4G8gHsoBpxphNLm3OB27DSvTjgb8ZY8aLSCLwBZBujKkSkSXAe8aYRcf7TE30fqq2Egqym4Znzs9yGaxtcPOSzuhke2PtTjpa6dI73XpesgcrXYwxVNQ2NF1obnE9wXVHUVJRQ2lFLa0UIxEgOHcIze9faHkz25H3kSGB3lmN5IaTvWFqHLDNGLPDubJXgYuATS5tLgJeMNZe4ysRiRaRI89iCwLCRaQOiAD2nODvoXxdSASkTbZe4BysbW3TTVyb3oLVz1vzeiY1T/zxp3br0rlO0ZFKl+TTofdM2ypdRISo0CCiQoNIiYtst31Do6GssvboGUGJy1lCsctOYUPBQYrLazhc3cqDPoDQoACX+xNczhCO2VFYZxQhQb5R/eNOok8EXHbt5GMdtbfXJtEYky0ifwbygCrgA2PMB619iIjMAeYAJCfr0Vy3EBhsXahNcsCkuVaFRuGmppLOnf+D9UusthHxVlXPkXr+viOtxziqYx2tdNnc/OXHY7oEBogzKYeCG8/qqKlvcJamtlGR5Cxf3brvMMUVtW12I/V0diPFtaxCajkURmQovcKDCWijG8mT92mAe4m+tchaniS12kZEYrCO9tOAMuA1EbnGGPPiMY2NeRp4GqyuGzfiUv4mIMDqu+87AsbPaRqs7Ug5Z95y2Pxvq21oz+aDtfUf0/0Ga+vomC6nntPU7eLDY7p0htCgQPr1Cqdfr/arfYwxlNfUH3szW4uKpB3F5WTl1lJaWUtrPeKBAUJMRMgxF5oLD1Xzwab91DVYCxWUVfHLpesBOi3Zu5Po84EBLu+TOLb7pa02ZwE7jTFFACKyFJgIHJPolTqGCMSdYr1Om25NO1jgUtmzHD6635oeFAaJzsHaUiZA0jgIjbIv9s7WckyXI8Pp6pguHici9AgLpkdYMKnxHetGOnq2cHSn0PTzuvwySsprOVxzbDdSVV0Djyzb2qWJPgsYLCJpQAFwFfDjFm3eBm519t+PBw4aY/aKSB5wuohEYHXdnAnoVVZ14nolNh+sraIEdn/VVNL5+V/gs5aDtU2y+px9YcyTo5Uum5rfZNRapYtrl0s3GtPF27l2I53ap/2dbFujo+5pZSC9E9VuojfG1IvIrcAyrPLKhcaYjSJyk3P+fOA9rIqbbVjllTOd81aKyOvAaqAeWIOze0apThEZB0MvsF4ANYedJZ3OIZq/fgZW/N2a1zu9aYTOlEnQs1/b6/W0jlS6uHa56Jgufqet0VH7R3feTWR6w5Tyb3XVsGd1U1fP7pVQW27Ni0lrXtkTk9b5CbS1SpfCzXBgp47pooCOj47aFh2PXnVfwWFNiRycg7Wtb0r8W9+HHOdDmKP6Nk/8CcOaJ9rD++D1mXDZIujRorSjWaXLkW6XNipd+gyHkZf7fKWL6hxHkrknq270iF51b42NVjI+MkLnruVwqMCaFxbdvKtnzT+tOv+RV8Coy91/epFWuqguoGPdKOUuY5oP1pa3Akq2td3+SKVLs6SulS6q62nXjVLuEoGYFOuVMc2atnc9vHcnFGRBY4M1SFvKJDjvYeg9xN54lXKDXulRqj39RlpH6sZY9fqmwart1ySvfIQe0SvljopCGDsTHDMh+zko3293REq5TRO9Uu646qWmny/8q31xKHUCtOtGKaX8nCZ6pZTyc5rolVLKz2miV0opP6eJXiml/JwmeqWU8nNeOQSCiBQBu05w8XiguBPD6SwaV8doXB2jcXWMP8aVYoxJaG2GVyb6kyEi2W2N92AnjatjNK6O0bg6prvFpV03Sinl5zTRK6WUn/PHRO+tjyrUuDpG4+oYjatjulVcftdHr5RSqjl/PKJXSinlQhO9Ukr5OZ9J9CIyRUS2isg2EbmnlfkiIo87568TkdPcXdbDcV3tjGediCwXkdEu83JFZL2I5IhIpz470Y24zhCRg87PzhGR37q7rIfjusslpg0i0iAisc55nvy+FopIoYhsaGO+XdtXe3HZtX21F5dd21d7cdm1fQ0QkU9EZLOIbBSRua208dw2Zozx+hcQCGwHBgIhwFogvUWb84H3AQFOB1a6u6yH45oIxDh/Pu9IXM73uUC8Td/XGcA7J7KsJ+Nq0f6HwMee/r6c654MnAZsaGN+l29fbsbV5duXm3F1+fblTlw2bl/9gNOcP/cAvunKHOYrR/TjgG3GmB3GmFrgVeCiFm0uAl4wlq+AaBHp5+ayHovLGLPcGHPA+fYrIKmTPvuk4vLQsp297mnAK5302cdljPkMKD1OEzu2r3bjsmn7cuf7aout31cLXbl97TXGrHb+fBjYDCS2aOaxbcxXEn0isNvlfT7HfklttXFnWU/G5Wo21h77CAN8ICKrRGROJ8XUkbgmiMhaEXlfRIZ3cFlPxoWIRABTgDdcJnvq+3KHHdtXR3XV9uWurt6+3Gbn9iUiqcAYYGWLWR7bxnzlUYLSyrSWdaFttXFn2RPl9rpF5PtYf4jfcZk8yRizR0R6A/8VkS3OI5KuiGs11tgY5SJyPvAWMNjNZT0Z1xE/BL40xrgenXnq+3KHHduX27p4+3KHHdtXR9iyfYlIFNbO5XZjzKGWs1tZpFO2MV85os8HBri8TwL2uNnGnWU9GRciMgp4FrjIGFNyZLoxZo/z30LgTaxTtC6JyxhzyBhT7vz5PSBYROLdWdaTcbm4ihan1R78vtxhx/blFhu2r3bZtH11RJdvXyISjJXkXzLGLG2liee2MU9ceOjsF9aZxw4gjaaLEcNbtLmA5hcyvnZ3WQ/HlQxsAya2mB4J9HD5eTkwpQvj6kvTDXPjgDznd2fr9+Vs1wurnzWyK74vl89Ipe2Li12+fbkZV5dvX27G1eXblztx2bV9OX/3F4DHjtPGY9uYT3TdGGPqReRWYBnWFeiFxpiNInKTc/584D2sq9bbgEpg5vGW7cK4fgvEAU+KCEC9sUan6wO86ZwWBLxsjPlPF8Z1GfATEakHqoCrjLVV2f19AVwMfGCMqXBZ3GPfF4CIvIJVKRIvIvnA74Bgl7i6fPtyM64u377cjKvLty834wIbti9gEjAdWC8iOc5pv8LaUXt8G9MhEJRSys/5Sh+9UkqpE6SJXiml/JwmeqWU8nOa6JVSys9poldKKT+niV4ppfycJnqllPJz/w9Wre2aQX5ERgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(losses)), losses, marker='o',label='losses')\n",
    "plt.plot(range(len(eval_losses)), eval_losses, marker='*',label='eval_losses')\n",
    "plt.legend() #图例\n",
    "plt.title(\"loss\") #标题\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39975994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型 .save('模型.h5')\n",
    "\n",
    "torch.save(model,'pytorch_number_CBAM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2df7c2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CNN_cbam                                 [1, 10]                   --\n",
       "├─Sequential: 1-1                        [1, 16, 14, 14]           --\n",
       "│    └─Conv2d: 2-1                       [1, 16, 28, 28]           160\n",
       "│    └─ReLU: 2-2                         [1, 16, 28, 28]           --\n",
       "│    └─MaxPool2d: 2-3                    [1, 16, 14, 14]           --\n",
       "├─Sequential: 1-2                        [1, 32, 7, 7]             --\n",
       "│    └─Conv2d: 2-4                       [1, 32, 14, 14]           4,640\n",
       "│    └─ReLU: 2-5                         [1, 32, 14, 14]           --\n",
       "│    └─MaxPool2d: 2-6                    [1, 32, 7, 7]             --\n",
       "├─cbam_block: 1-3                        [1, 32, 7, 7]             --\n",
       "│    └─ChannelAttention: 2-7             [1, 32, 1, 1]             --\n",
       "│    │    └─AdaptiveMaxPool2d: 3-1       [1, 32, 1, 1]             --\n",
       "│    │    └─AdaptiveAvgPool2d: 3-2       [1, 32, 1, 1]             --\n",
       "│    │    └─Sequential: 3-3              [1, 32]                   256\n",
       "│    │    └─Sequential: 3-4              [1, 32]                   (recursive)\n",
       "│    │    └─Sigmoid: 3-5                 [1, 32]                   --\n",
       "│    └─SpatialAttention: 2-8             [1, 1, 7, 7]              --\n",
       "│    │    └─Conv2d: 3-6                  [1, 1, 7, 7]              98\n",
       "│    │    └─Sigmoid: 3-7                 [1, 1, 7, 7]              --\n",
       "├─Linear: 1-4                            [1, 10]                   15,690\n",
       "==========================================================================================\n",
       "Total params: 20,844\n",
       "Trainable params: 20,844\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 1.06\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.15\n",
       "Params size (MB): 0.08\n",
       "Estimated Total Size (MB): 0.24\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = CNN_cbam()\n",
    "\n",
    "\n",
    "summary(model, (1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ff6c5",
   "metadata": {},
   "source": [
    "# 5.模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b358f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchkeras import summary,Model\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aee0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "\n",
    "#方法一训练模型\n",
    "model = torch.load('pytorch_number_ecan.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b109cba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#方法二训练模型\n",
    "model_2 = torch.load('pytorch_number_CBAM.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9238dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片原始格式: (4, 28, 28)\n",
      "标签： [1 0 4 1]\n",
      "图片转换成tensor格式后： torch.Size([4, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 手动导入预测数据\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() \n",
    "\n",
    "# 取前4张图片\n",
    "test_images = test_images[2:6]\n",
    "test_labels = test_labels[2:6]\n",
    "print('图片原始格式:', test_images.shape)\n",
    "print('标签：', test_labels)\n",
    "\n",
    "# 图片格式转换成tensor格式\n",
    "test_images = torch.tensor(test_images)\n",
    "print('图片转换成tensor格式后：', test_images.shape)\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(test_images[i])\n",
    "plt.show\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3a0208ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 28, 28])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "# 图片类型转换成FloatTensor(预测和训练数据类型一致)\n",
    "test_images = test_images.type(torch.FloatTensor)/255\n",
    "\n",
    "print(test_images.shape)\n",
    "print(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72140cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "添加通道维度 torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 为图片格式添加通道维\n",
    "pic = test_images[0:4]\n",
    "\n",
    "pic = pic.reshape(4,1,28,28)\n",
    "print('添加通道维度', pic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6ee79",
   "metadata": {},
   "source": [
    "#### 方法一模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e81d8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测模式\n",
    "\n",
    "output = model(pic)\n",
    "print(output)\n",
    "\n",
    "print('------')\n",
    "# 按行找到一行内值最大的列号\n",
    "prediction = torch.max(output, dim=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50953cf",
   "metadata": {},
   "source": [
    "#### 方法二模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918258ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_2(pic)\n",
    "print(output)\n",
    "\n",
    "print('------')\n",
    "# 按行找到一行内值最大的列号\n",
    "prediction = torch.max(output, dim=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2fbdfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bc08bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a956d50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
