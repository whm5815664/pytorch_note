{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "vocational-assessment",
   "metadata": {},
   "source": [
    "# 手写数字识别\n",
    "\n",
    "\n",
    "### 问题：分类问题（10类）\n",
    "\n",
    "### 输入：灰度图像（28×28个像素）\n",
    "\n",
    "### 输出：分类0-9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd78c8d8",
   "metadata": {},
   "source": [
    "# 0.超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "513206fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "每次在训练集中提取64张图像进行批量化训练，目的是提高训练速度。\n",
    "就好比搬砖，一次搬一块砖头的效率肯定要比一次能搬64块要低得多\n",
    "\"\"\"\n",
    "BATCH_SIZE = 64\n",
    "#学习率，学习率一般为0.01，0.1等等较小的数，为了在梯度下降求解时避免错过最优解\n",
    "LR = 0.001\n",
    "\"\"\"\n",
    "EPOCH 假如现在我有1000张训练图像，因为每次训练是64张，\n",
    "每当我1000张图像训练完就是一个EPOCH，训练多少个EPOCH自己决定\n",
    "\"\"\"\n",
    "EPOCH = 1\n",
    "\"\"\"\n",
    "现在我要训练的训练集是系统自带的，需要先下载数据集，\n",
    "当DOWNLOAD_MNIST为True是表示学要下载数据集，一但下载完，保存\n",
    "然后这个参数就可以改为False，表示不用再次下载\n",
    "\"\"\"\n",
    "DOWNLOAD_MNIST = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statistical-great",
   "metadata": {},
   "source": [
    "# 1.导入数据\n",
    "\n",
    "### 原始数据（来自keras.datasets.mnist）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "monetary-excellence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 手动导入数据\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() #mnist.load_data('路径')为下载并保存数据集位置，默认位置在C:\\Users\\管理员\\.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98fb8fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# pytorch导入\\n\\nimport torch\\nimport torchvision\\nfrom torch.autograd import Variable\\nfrom torch.utils.data import Dataset, TensorDataset, DataLoader\\n\\n#训练集\\n# 读取\\ntrain_data = torchvision.datasets.MNIST(\\n    root=\\'./mnist\\',\\n    train = True,\\n    transform=torchvision.transforms.ToTensor(),\\n    download=DOWNLOAD_MNIST\\n)\\n\\n# 划分\\ntrain_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2 )\\n#每个batch_size的shape为[64, 1, 28, 28]\\nprint(\"样本\")\\nprint(train_data.train_data.shape)\\nprint(train_data.train_data[:3])\\nprint(\"标签\")\\nprint(train_data.train_labels.shape)\\nprint(train_data.train_labels[:3])\\n\\n\\n\\n# 测试集\\n# 读取\\ntest_data = torchvision.datasets.MNIST(\\n    root=\\'./mnist\\',\\n    train = False,\\n)\\n\\n# 处理\\n\\ntest_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000]/255.0\\n\"\"\"\\ntest_data.test_data中的shape为[10000, 28, 28]代表1w张图像，都是28x28，当时并未表明channels,因此在unsqueeze在1方向想加一个维度，\\n则shape变为[10000, 1, 28, 28]，然后转化为tensor的float32类型，取1w张中的2000张，并且将其图片进行归一化处理，避免图像几何变换的影响\\n\"\"\"\\n#标签取前2000\\ntest_y = test_data.test_labels[:2000]\\n\\nprint(\"样本\")\\nprint(test_x.shape)\\nprint(test_x[:3])\\nprint(\"标签\")\\nprint(test_y.shape)\\nprint(test_y[:3])\\n\\n\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# pytorch导入\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "#训练集\n",
    "# 读取\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist',\n",
    "    train = True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=DOWNLOAD_MNIST\n",
    ")\n",
    "\n",
    "# 划分\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=2 )\n",
    "#每个batch_size的shape为[64, 1, 28, 28]\n",
    "print(\"样本\")\n",
    "print(train_data.train_data.shape)\n",
    "print(train_data.train_data[:3])\n",
    "print(\"标签\")\n",
    "print(train_data.train_labels.shape)\n",
    "print(train_data.train_labels[:3])\n",
    "\n",
    "\n",
    "\n",
    "# 测试集\n",
    "# 读取\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root='./mnist',\n",
    "    train = False,\n",
    ")\n",
    "\n",
    "# 处理\n",
    "\n",
    "test_x = Variable(torch.unsqueeze(test_data.test_data, dim=1), volatile=True).type(torch.FloatTensor)[:2000]/255.0\n",
    "\"\"\"\n",
    "test_data.test_data中的shape为[10000, 28, 28]代表1w张图像，都是28x28，当时并未表明channels,因此在unsqueeze在1方向想加一个维度，\n",
    "则shape变为[10000, 1, 28, 28]，然后转化为tensor的float32类型，取1w张中的2000张，并且将其图片进行归一化处理，避免图像几何变换的影响\n",
    "\"\"\"\n",
    "#标签取前2000\n",
    "test_y = test_data.test_labels[:2000]\n",
    "\n",
    "print(\"样本\")\n",
    "print(test_x.shape)\n",
    "print(test_x[:3])\n",
    "print(\"标签\")\n",
    "print(test_y.shape)\n",
    "print(test_y[:3])\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-craps",
   "metadata": {},
   "source": [
    "# 2.创建自己的Datasets数据集\n",
    "\n",
    "from torch.utils.data import Dataset, TensorDataset ,DataLoader\n",
    "\n",
    "\n",
    ">1.将数据转为tensor格式\n",
    ">>`数据 = torch.tensor(mumpy数据)` \n",
    "\n",
    "\n",
    ">2.数据处理\n",
    ">>图像数据处理：\n",
    ">>>**图像数据列表维度shape：[图像数量,通道维数,图像长像素,图像宽像素]**  \n",
    ">>>缺少通道维黑白图像处理:`图片样本data = Variable(torch.unsqueeze(图片样本data, dim=1), volatile=True).type(torch.FloatTensor)/255`  \n",
    ">>>数据类型转换:`数据变量 = 数据变量.type(torch.FloatTensor)`\n",
    "\n",
    ">>标签处理\n",
    ">>>转换为one-hot编码:`标签labels = utils.to_categorical(标签labels)`  \n",
    ">>>标签转换成long数据格式：`标签labels = 标签labels.long()`\n",
    "\n",
    ">3.创建数据集\n",
    ">>`数据集 = TensorDataset(样本data, 标签labels)`\n",
    "\n",
    ">4.加载数据集\n",
    ">>`train_loader = DataLoader(train_dataset, batch_size=120)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d962467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 对分类标签y进行one-hot编码  utils.to_categorical(标签列表, num_classes=标签类别数, dtype=\\'编码后标签格式\\')\\nfrom tensorflow.keras import utils\\n\\nprint(\"编码前\")\\nprint(train_labels)\\n\\ntrain_labels = utils.to_categorical(train_labels)\\ntest_labels = utils.to_categorical(test_labels)\\n\\nprint(\"编码后\")\\nprint(train_labels)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "# 对分类标签y进行one-hot编码  utils.to_categorical(标签列表, num_classes=标签类别数, dtype='编码后标签格式')\n",
    "from tensorflow.keras import utils\n",
    "\n",
    "print(\"编码前\")\n",
    "print(train_labels)\n",
    "\n",
    "train_labels = utils.to_categorical(train_labels)\n",
    "test_labels = utils.to_categorical(test_labels)\n",
    "\n",
    "print(\"编码后\")\n",
    "print(train_labels)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2900642b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n",
      "torch.Size([60000, 28, 28])\n",
      "torch.Size([60000, 1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-5-8705a23add60>:23: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  train_images = Variable(torch.unsqueeze(train_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n",
      "<ipython-input-5-8705a23add60>:24: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  test_images = Variable(torch.unsqueeze(test_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "print(train_images)\n",
    "\n",
    "# 1.把数据转换成tensor格式\n",
    "train_images = torch.tensor(train_images)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "\n",
    "test_images = torch.tensor(test_images)\n",
    "test_labels = torch.tensor(test_labels)\n",
    "\n",
    "print(train_images.shape)\n",
    "\n",
    "\n",
    "# 2. 数据处理\n",
    "# 将标签转换成long格式\n",
    "train_labels = train_labels.long()\n",
    "test_labels = test_labels.long()\n",
    "\n",
    "# 图像数据调整增加维度 [图片数, 长, 宽]->[图片数, 通道数, 长, 宽], 将数据转为tensor的Float格式\n",
    "train_images = Variable(torch.unsqueeze(train_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n",
    "test_images = Variable(torch.unsqueeze(test_images, dim=1), volatile=True).type(torch.FloatTensor)/255\n",
    "\n",
    "print(train_images.shape)\n",
    "\n",
    "\n",
    "# 3.创建数据集\n",
    "train_dataset = TensorDataset(train_images, train_labels)\n",
    "test_dataset = TensorDataset(test_images, test_labels)\n",
    "\n",
    "\n",
    "# 4.加载数据集\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "test_loader =DataLoader(test_dataset, batch_size=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "analyzed-duration",
   "metadata": {},
   "source": [
    "# 3.构建网络\n",
    "\n",
    "```python\n",
    "# 导入包\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 定义神经网络类\n",
    "class 自定义神经网络类名(nn.Module):\n",
    "    # 可学习参数的层（如全连接层、卷积层等）\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.自定义layer名1 = nn.layer层(参数)\n",
    "        self.自定义layer名2 = nn.layer层(参数)\n",
    "        self.自定义layer名3 = nn.Sequential(\n",
    "            nn.layer层(参数)，\n",
    "            nn.layer层(参数)\n",
    "        ）\n",
    "        \n",
    "    # 实现模型的功能，实现各个层之间的连接关系\n",
    "    # nn.functional实现不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)的构造\n",
    "    def forward(self, x):\n",
    "        x = self.自定义layer名1(x)\n",
    "        x = F.不可学习参数层(x)\n",
    "        x = self.自定义layer名2(x)\n",
    "        x = 不可学习参数层(自定义layer名3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 实例化神经网络\n",
    "model实例 = 自定义神经网络类名()\n",
    "\n",
    "\n",
    "# 保存模型\n",
    "torch.save(model实例, '存储路径')\n",
    "\n",
    "# 加载模型\n",
    "model = torch.load(\"model.pth\")\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-marble",
   "metadata": {},
   "source": [
    "#### ·可学习layer\n",
    "\n",
    ">**卷积层**\n",
    ">>nn.Conv2d(\n",
    ">>&nbsp;&nbsp;&nbsp;  in_channels = 输入特征矩阵的深度,   \n",
    ">>&nbsp;&nbsp;&nbsp;  out_channels = 卷积核数|输出特征矩阵深度,   \n",
    ">>&nbsp;&nbsp;&nbsp;  kernel_size = (卷积核长, 卷积核宽),   \n",
    ">>&nbsp;&nbsp;&nbsp;  stride = 卷积框步长,   \n",
    ">>&nbsp;&nbsp;&nbsp;  padding = (填充上下行数, 填充左右列数),   \n",
    ">>&nbsp;&nbsp;&nbsp;  dilation = 卷积核元素之间的间距1,   \n",
    ">>&nbsp;&nbsp;&nbsp;  groups = 从输入通道到输出通道的阻塞连接数1,   \n",
    ">>&nbsp;&nbsp;&nbsp;  bias = 添加偏置T/F,   \n",
    ">>&nbsp;&nbsp;&nbsp;  padding_mode = '填充数字zeros'  \n",
    ">>)\n",
    "\n",
    ">>说明：\n",
    ">>>in_channels = 输入通道维的元素数  \n",
    ">>>图像(通道，图像长，图像宽)（黑白图像通道=1，RGB图像通道=3）   \n",
    ">>>out_channels = 提取特征数 = 输出特征矩阵深度 = 输出特征矩阵通道维\n",
    "\n",
    ">**全连接层**\n",
    ">>nn.Linear(in_features=每个输入样本的大小, out_features=每个输出样本的大小)  \n",
    ">>说明：\n",
    "\n",
    "\n",
    "#### ·不可学习layer\n",
    ">**将多维数据转成一维**\n",
    ">>x.view(x.size(0), -1)  \n",
    ">>说明：在卷积层转全连接层之间使用，用在forward(self, x)中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "binding-pledge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 主干网络\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# 定义神经网络类\n",
    "class CNN(nn.Module):\n",
    "    # 可学习参数的层（如全连接层、卷积层等）\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 第一部分卷积层1\n",
    "        self.conv1 = nn.Sequential(\n",
    "            # 卷积层(输入通道维1，输出通道维16，卷积窗口3*3)\n",
    "            nn.Conv2d(1, 16, kernel_size=(3,3), stride=1, padding=1),  # 维度变换(1,28,28) （黑白图像1通道，长28像素，宽28像素）->(16,28,28) （16个卷积核提取16个特征通道，长28，宽28）图像边缘扩展，没被卷积抛去\n",
    "            # 激活函数\n",
    "            nn.ReLU(),\n",
    "            # 池化层\n",
    "            nn.MaxPool2d(2) # 维度变化(16,28,28)->(16,14,14)\n",
    "        )\n",
    "            \n",
    "        #第二部分卷积层2\n",
    "        self.conv2 = nn.Sequential(\n",
    "            # 卷积层(输入通道维16，输出通道维32，卷积窗口3*3)\n",
    "            nn.Conv2d(16, 32, kernel_size=(3,3), stride=1, padding=1),  # 维度变换(16,14,14)->(32,14,14)\n",
    "            # 激活函数\n",
    "            nn.ReLU(),\n",
    "            # 池化层\n",
    "            nn.MaxPool2d(2) # 维度变化(32,14,14)->(32,7,7)\n",
    "        )\n",
    "            \n",
    "        # 全连接层\n",
    "        self.out = nn.Linear(32*7*7, 10)\n",
    "            \n",
    "            \n",
    "        \n",
    "    # 实现模型的功能，实现各个层之间的连接关系\n",
    "    # nn.functional实现不具有可学习参数的层(如ReLU、dropout、BatchNormanation层)的构造\n",
    "    def forward(self, x):\n",
    "        # 执行卷积层1 conv1\n",
    "        x = self.conv1(x)\n",
    "        # 执行卷积层2 conv2\n",
    "        x = self.conv2(x)\n",
    "        # 将图像数据转为1维\n",
    "        x = x.view(x.size(0),-1)\n",
    "        # 执行全连接层 out\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dfa53c7",
   "metadata": {},
   "source": [
    "# 4.训练模型\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2039f4b1",
   "metadata": {},
   "source": [
    "#### 方法一 ：利用torchkeras包训练\n",
    "\n",
    "(需要torchkeras支持)  \n",
    "import from torchkeras import summary,Model \n",
    "\n",
    "1.实例化模型  \n",
    "```\n",
    "model = Model(自定义神经网络类名())\n",
    "```\n",
    "\n",
    "2.编译模型\n",
    "```\n",
    "model.compile(loss_func = 损失函数,\n",
    "             optimizer= 优化方法,\n",
    "             metrics_dict={\"accuracy\":accuracy})\n",
    "```\n",
    "\n",
    "3.训练模型\n",
    "```\n",
    "dfhistory = model.fit(训练次数,train_loader, test_loader, log_step_freq=100) \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b593b836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (net): CNN(\n",
      "    (conv1): Sequential(\n",
      "      (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (conv2): Sequential(\n",
      "      (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (1): ReLU()\n",
      "      (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    )\n",
      "    (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 28, 28]             160\n",
      "              ReLU-2           [-1, 16, 28, 28]               0\n",
      "         MaxPool2d-3           [-1, 16, 14, 14]               0\n",
      "            Conv2d-4           [-1, 32, 14, 14]           4,640\n",
      "              ReLU-5           [-1, 32, 14, 14]               0\n",
      "         MaxPool2d-6             [-1, 32, 7, 7]               0\n",
      "            Linear-7                   [-1, 10]          15,690\n",
      "================================================================\n",
      "Total params: 20,490\n",
      "Trainable params: 20,490\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.002991\n",
      "Forward/backward pass size (MB): 0.323074\n",
      "Params size (MB): 0.078163\n",
      "Estimated Total Size (MB): 0.404228\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from torchkeras import summary,Model\n",
    "\n",
    "# 实例化模型\n",
    "model = Model(CNN())\n",
    "model = model.float()\n",
    "\n",
    "# 查看模型\n",
    "print(model)\n",
    "print(summary(model, input_shape=(1,28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69746c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(y_pred,y_true):\n",
    "    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data\n",
    "    return accuracy_score(y_true.numpy(),y_pred_cls.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71543cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss_func = nn.CrossEntropyLoss(),\n",
    "             optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),\n",
    "             metrics_dict={\"accuracy\":accuracy})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57bbfa11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training ...\n",
      "\n",
      "================================================================================2022-07-04 15:55:44\n",
      "{'step': 100, 'loss': 0.451, 'accuracy': 0.853}\n",
      "{'step': 200, 'loss': 0.293, 'accuracy': 0.905}\n",
      "{'step': 300, 'loss': 0.241, 'accuracy': 0.923}\n",
      "{'step': 400, 'loss': 0.211, 'accuracy': 0.934}\n",
      "{'step': 500, 'loss': 0.19, 'accuracy': 0.94}\n",
      "{'step': 600, 'loss': 0.176, 'accuracy': 0.944}\n",
      "{'step': 700, 'loss': 0.167, 'accuracy': 0.947}\n",
      "{'step': 800, 'loss': 0.159, 'accuracy': 0.95}\n",
      "{'step': 900, 'loss': 0.151, 'accuracy': 0.953}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   1   | 0.149 |  0.953   |  0.068   |     0.98     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2022-07-04 15:56:01\n",
      "{'step': 100, 'loss': 0.081, 'accuracy': 0.978}\n",
      "{'step': 200, 'loss': 0.084, 'accuracy': 0.976}\n",
      "{'step': 300, 'loss': 0.085, 'accuracy': 0.975}\n",
      "{'step': 400, 'loss': 0.086, 'accuracy': 0.975}\n",
      "{'step': 500, 'loss': 0.086, 'accuracy': 0.975}\n",
      "{'step': 600, 'loss': 0.086, 'accuracy': 0.975}\n",
      "{'step': 700, 'loss': 0.088, 'accuracy': 0.974}\n",
      "{'step': 800, 'loss': 0.088, 'accuracy': 0.974}\n",
      "{'step': 900, 'loss': 0.089, 'accuracy': 0.974}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   2   | 0.089 |  0.974   |  0.069   |    0.978     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2022-07-04 15:56:17\n",
      "{'step': 100, 'loss': 0.065, 'accuracy': 0.981}\n",
      "{'step': 200, 'loss': 0.08, 'accuracy': 0.977}\n",
      "{'step': 300, 'loss': 0.079, 'accuracy': 0.978}\n",
      "{'step': 400, 'loss': 0.083, 'accuracy': 0.976}\n",
      "{'step': 500, 'loss': 0.083, 'accuracy': 0.976}\n",
      "{'step': 600, 'loss': 0.081, 'accuracy': 0.977}\n",
      "{'step': 700, 'loss': 0.084, 'accuracy': 0.976}\n",
      "{'step': 800, 'loss': 0.085, 'accuracy': 0.975}\n",
      "{'step': 900, 'loss': 0.084, 'accuracy': 0.976}\n",
      "\n",
      " +-------+-------+----------+----------+--------------+\n",
      "| epoch |  loss | accuracy | val_loss | val_accuracy |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "|   3   | 0.085 |  0.975   |  0.066   |     0.98     |\n",
      "+-------+-------+----------+----------+--------------+\n",
      "\n",
      "================================================================================2022-07-04 15:56:35\n",
      "Finished Training...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dfhistory = model.fit(3,train_loader, test_loader, log_step_freq=100) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e561eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型 .save('模型.h5')\n",
    "\n",
    "torch.save(model,'pytorch_number_model.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460e1ba",
   "metadata": {},
   "source": [
    "#### 方法二：手动训练\n",
    "\n",
    "1.实例化模型\n",
    "```\n",
    "model变量 = 自定义神经网络类名()\n",
    "```\n",
    "\n",
    "2.定义损失函数和优化器\n",
    "```\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model变量.parameters(), lr = 学习率) \n",
    "```\n",
    "\n",
    "3.训练模型\n",
    "```\n",
    "for epoch in range(训练轮数):  # 训练轮数\n",
    "    \n",
    "    # 遍历训练集每条数据，进行训练 \n",
    "    running_loss = 0.0 # 记录一轮中每条训练数据预测的损失累加\n",
    "    for step, (x, y) in enumerate(训练集加载train_loader):   #【enumerate()枚举对象 得到格式（id，元素）】\n",
    "        b_x = Variable(x) # 数据x\n",
    "        b_y = Variable(y) # 标签y\n",
    "    \n",
    "        output = model_2(b_x) # 把数据输入进网络\n",
    "        loss = loss_func(output, b_y) # 计算一条数据的损失\n",
    "        running_loss += loss.item() # 损失累加\n",
    "        \n",
    "        optimizer.zero_grad() # 梯度置零\n",
    "        loss.backward()  # loss反向传播\n",
    "        optimizer.step() # 反向传播后参数更新\n",
    "        \n",
    "    \n",
    "    print('训练轮数：', epoch, ' 平均损失：',running_loss/len(train_loader)) #平均损失=每轮每条训练数据损失求和/每轮训练数据数\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83c03e60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (out): Linear(in_features=1568, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "model_2 = CNN()\n",
    "\n",
    "# 查看模型\n",
    "print(model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6d52593",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 损失函数\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model_2.parameters(), lr = 0.02) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1a3c953b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练轮数： 0  训练平均损失loss： 0.22131887940801123\n",
      "训练轮数： 0  测试平均损失val_loss： 0.12813471588084385\n",
      "训练轮数： 1  训练平均损失loss： 0.13678165808119483\n",
      "训练轮数： 1  测试平均损失val_loss： 0.12506504611311747\n",
      "训练轮数： 2  训练平均损失loss： 0.13559626766553978\n",
      "训练轮数： 2  测试平均损失val_loss： 0.1099744760514503\n",
      "end\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "EPOCH = 3 # 训练轮数\n",
    "\n",
    "#记录用于绘图\n",
    "losses = []#记录每次迭代后训练的loss\n",
    "eval_losses = []#测试的\n",
    "\n",
    "\n",
    "\n",
    "# 自定义训练方法\n",
    "\n",
    "for epoch in range(EPOCH):  # 训练轮数\n",
    "    # 遍历训练集每条数据，进行训练，得到每轮损失loss\n",
    "    running_loss = 0.0\n",
    "    for step, (x, y) in enumerate(train_loader):   #【enumerate()枚举对象 得到格式（id，元素）】\n",
    "        b_x = Variable(x) # 数据x\n",
    "        b_y = Variable(y) # 标签y\n",
    "    \n",
    "        output = model_2(b_x) # 把数据输入进网络\n",
    "        loss = loss_func(output, b_y) # 计算一条数据的损失\n",
    "        running_loss += loss.item() # 损失累加\n",
    "        \n",
    "        optimizer.zero_grad() # 梯度置零\n",
    "        loss.backward()  # loss反向传播\n",
    "        optimizer.step() # 反向传播后参数更新\n",
    "    \n",
    "    losses.append(running_loss/len(train_loader)) # 记录该轮平均损失，后续用于画图\n",
    "    print('训练轮数：', epoch, ' 训练平均损失loss：',running_loss/len(train_loader)) #平均损失=每轮每条训练数据损失求和/每轮训练数据数\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 遍历测试集每条数据，进行测试，每轮训练后损失val_loss\n",
    "    running_loss = 0.0\n",
    "    for step, (x, y) in enumerate(test_loader): \n",
    "        b_x = Variable(x) # 数据x\n",
    "        b_y = Variable(y) # 标签y\n",
    "        \n",
    "        output = model_2(b_x) # 把数据输入进网络\n",
    "        loss = loss_func(output, b_y) # 计算一条数据的损失\n",
    "        running_loss += loss.item() # 损失累加\n",
    "    \n",
    "    eval_losses.append(running_loss/len(test_loader)) # 记录该轮平均损失，后续用于画图\n",
    "    print('训练轮数：', epoch, ' 测试平均损失val_loss：',running_loss/len(test_loader)) #平均损失=每轮每条训练数据损失求和/每轮训练数据数\n",
    "    \n",
    "        \n",
    "    \n",
    "print('end') \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dc3433e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 画图\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(range(len(losses)), losses, marker='o',label='losses')\n",
    "plt.plot(range(len(eval_losses)), eval_losses, marker='*',label='eval_losses')\n",
    "plt.legend() #图例\n",
    "plt.title(\"loss\") #标题\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55d33dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型 .save('模型.h5')\n",
    "\n",
    "torch.save(model_2,'pytorch_number_model_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1ff6c5",
   "metadata": {},
   "source": [
    "# 5.模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b358f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "from torchkeras import summary,Model\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# 加载模型\n",
    "\n",
    "#方法一训练模型\n",
    "model = torch.load('pytorch_number_model.h5')\n",
    "#方法二训练模型\n",
    "model_2 = torch.load('pytorch_number_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9238dfb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "图片原始格式: (4, 28, 28)\n",
      "标签： [1 0 4 1]\n",
      "图片转换成tensor格式后： torch.Size([4, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(close=None, block=None)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATsAAAD7CAYAAAAVQzPHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAAW6ElEQVR4nO3de3RV1Z0H8O+PJCQCWok8jCEKrUHBR1HDo2pblaJobUFbH3TGMh1abMGqq85S2tqHtrXYzmJ1fBsLE6YqasUW2mo7mIGi1fIUEYgQVNBgJFB5RAohCb/5I8dz7r7mJDf3nse9d38/a2Xdve8+yf5pfvxyzr7nIaoKIqJ81yvuAIiIosBiR0RWYLEjIiuw2BGRFVjsiMgKLHZEZIWMip2ITBSRzSKyVURmBRUUUdyY2/lH0j3PTkQKAGwBMAFAA4BVAKao6qbgwiOKHnM7PxVm8L1jAGxV1TcBQESeADAJgG9C9JZiLUHfDKakoDRjz25VHRh3HFmKuZ2jDuEADmuLdDaWSbErB/BOQr8BwNiuvqEEfTFWxmcwJQXleX16e9wxZDHmdo5aobW+Y5kUu86q50eOiUVkOoDpAFCCPhlMRxQZ5nYeyuQDigYAFQn9IQDeTd5IVatVtUpVq4pQnMF0RJFhbuehTIrdKgCVIjJMRHoDuBbA4mDCIooVczsPpX0Yq6ptInIDgL8AKAAwT1U3BhYZUUyY2/kpkzU7qOqzAJ4NKBairMHczj+8goKIrMBiR0RWYLEjIiuw2BGRFVjsiMgKLHZEZAUWOyKyQkbn2dFHyTmnue0/Lf6NMXbGQze47YqfvBRZTEQfKjj2Y0Z/830fd9uvX/hrY+z2pnOM/mv/Mtxtt2/aEkJ04eKeHRFZgcWOiKzAYkdEVuCaXcCaRh/jttvQboz1eTe9W+ATBeXIsCFG/7ULHnbbrUnp+dNBa4z+J684121XcM2OiCg7sdgRkRV4GBuwPWd6h64NbS3G2HFzX446HCIUVniHrsOqt8YYSby4Z0dEVmCxIyIrsNgRkRW4ZpchPW+U0X/h8jlu+7PLv22MnYxXogiJLPf2D881+udM9J7t/YuyF9L+uf3O3eW23/mBOceA9W1u+6hFK9OeI0zcsyMiK7DYEZEVeBibofdHHmX0ywq8J8OXP10UdThEWH/9vUa/Vdt9tuyZZZ98zOt80hz73YEytz2vebIxVvh/5pUYceGeHRFZgcWOiKzAYkdEVuCaXYbGzzAvAfv9gWPddr9lm42xYFZOiD6qaJm3ZlYkBYH8zFcOHzH621oHuu0r+r5vjF3dr8lr/6baGLu83LzjcVy63bMTkXki0iQiGxLeKxWRJSJS77z2DzdMouAxt+2SymFsDYCJSe/NAlCrqpUAap0+Ua6pAXPbGt0exqrqchEZmvT2JAAXOO35AJYBuC3IwLJVwWmnGP27Bi0w+nP3e3eYaN+7L5KYKD25nNsHJ48x+l8r+63bTj7VJNVTT06v/abRH1hbbPSL93k/57sXmPtJr111j+/Pbfiud7XFkJ/H96CpdD+gGKyqjQDgvA4KLiSiWDG381ToH1CIyHQA0wGgBH262ZoodzC3c0u6e3Y7RaQMAJzXJr8NVbVaVatUtaoIxX6bEWUL5naeSnfPbjGAqQBmO6+LAosoy+2YcFyX42uaT0roHQw3GApD1uZ24nrxT+eYp3dU9T6cuKXvz0i8rAsAbl/6Jbc94tbXjbH2/ft9f84p9cON/sovlrjtMcWHjLHnvvULt31xya3G2NC7vEvJtMW8s3fQUjn1ZAGAlwGcIiINIjINHYkwQUTqAUxw+kQ5hbltl1Q+jZ3iMzQ+4FiIIsXctguvoOih/SNbuxxfd98ot30s+IAdCs6R3t4/V/OwtWv/vt07lbD5GvMuPcMbvBtt9uQKn/ak58bOqPFOW1l9/a+MsbICb86108yxLz0z1W3rq3U9iKDneG0sEVmBxY6IrMBiR0RW4JpdClouHe22F11s3gX2zt3mHR1KF6532+Y9I4ii8b2dVUZ//9e906XaG+pDmXPowt1u+weTxxljs49fFcqcPcU9OyKyAosdEVmBh7EpaLjI+990Zu8SY2zqtjOM/qAD5lnoRGHo6gad68/WpHfCOXQ1iLjNwl7mAk5Xsb57h9c+fnLQQZm4Z0dEVmCxIyIrsNgRkRW4ZpeCgad7d/lpV3M9onARH1FA0dj8Le+eeUE9+Doo2670Tm95euBKY6xVCxLaZtwn/Mhrh32qFvfsiMgKLHZEZAUWOyKyAtfsOlE47CSj/5+neE9uemRfhTFWOo+3caJo3P7pP8Q6f2GF9+S85nNOMMYe+toDKf2MlS3meapyuC3zwFLEPTsisgKLHRFZgYexnai/3txFH5fw4KhvrL3QGKvAhihCIordpjuOd9sbL74v5e9b+MEAt/3gf1xljJXUrUzePDTcsyMiK7DYEZEVWOyIyApcs+vEkYpDvmMH95b4jhHlk6Jl5gO1f162MK2fU7PjXLdd8ofo1uiScc+OiKzAYkdEVuBhbCceGPuo71j5c/53XSUKU4F49wXp6u6/+78yznfsjjvnGv0Lj/Jfskmew7xjSer/DvSiHSlvGybu2RGRFbotdiJSISJLRaRORDaKyE3O+6UiskRE6p1X3tiNcgpz2y6p7Nm1AbhFVUcAGAdgpoiMBDALQK2qVgKodfpEuYS5bZFu1+xUtRFAo9NuFpE6AOUAJgG4wNlsPoBlAG4LJcoIHPrCGLd9fknyx+Nc2sxHuZbbs5/8stu+etqvfLdb/sv7jX5XdzVuTX4QWRdSvTvy6bXfNPqVWJv6JCHq0ZqdiAwFcBaAFQAGO8nyYdIMCjw6oogwt/NfysVORPoBWAjgZlXd34Pvmy4iq0VkdSta0omRKFTMbTukdHwmIkXoSIbHVPUZ5+2dIlKmqo0iUgagqbPvVdVqANUAcIyU9mCnOVpvf9ELrVjM/y137vYehN1v0RpjLGv/gygluZTbH39yt9te+a/mlTxjiv1PIQlK4o03q9/7rDG2Z4Z3R5RT39pqjGXLo4FS+TRWAMwFUKeqcxKGFgOY6rSnAlgUfHhE4WFu2yWVPbvzAFwH4DURWee89z0AswE8JSLTALwN4KrOv50oazG3LZLKp7EvAhCf4fHBhkMUHea2Xaw9p6LgmGOM/m3nPeu77ePPfcZtf7yND9iheLRv2uK2f/idrxtj73zBu5Rsy6UPhzL/jHneKSUVP3spaXRPKHMGiZeLEZEVWOyIyArWHsYeaTHPi9r0T+8hO5/bUWWMVd610W1ny8foZLejFplX+QxP+Lz4M1NmGmNF/7bTbf/5tCeNsYs3XOu2j9SY505r0mrm0HW73HYu/jvgnh0RWYHFjoiswGJHRFawds1Ok9bsNics0/XGdmMsF9cnyF7HLPi7+cYCr3kFxhhDffFmQu9NdCXX/x1wz46IrMBiR0RWYLEjIiuw2BGRFVjsiMgKLHZEZAUWOyKyAosdEVmBxY6IrMBiR0RWYLEjIiuw2BGRFVjsiMgKohrdY55FZBeA7QAGANjdzeZRsTWWk1R1YERz5T0ntw8ge3IJsDO3ffM60mLnTiqyWlWrut8yfIyFgpJtv79siicbYuFhLBFZgcWOiKwQV7GrjmnezjAWCkq2/f6yKZ7YY4llzY6IKGo8jCUiK0Ra7ERkoohsFpGtIjIryrmd+eeJSJOIbEh4r1RElohIvfPaP6JYKkRkqYjUichGEbkpzngoM3HmNvM6NZEVOxEpAHA/gEsBjAQwRURGRjW/owbAxKT3ZgGoVdVKALVOPwptAG5R1REAxgGY6fz/iCseSlMW5HYNmNfdinLPbgyArar6pqoeBvAEgEkRzg9VXQ7g/aS3JwGY77TnA5gcUSyNqrrWaTcDqANQHlc8lJFYc5t5nZooi105gHcS+g3Oe3EbrKqNQMcvCsCgqAMQkaEAzgKwIhvioR7LxtyOPY+yLa+jLHbSyXvWfxQsIv0ALARws6rujzseSgtzO0k25nWUxa4BQEVCfwiAdyOc389OESkDAOe1KaqJRaQIHQnxmKo+E3c8lLZszG3mdZIoi90qAJUiMkxEegO4FsDiCOf3sxjAVKc9FcCiKCYVEQEwF0Cdqs6JOx7KSDbmNvM6mapG9gXgMgBbALwB4PtRzu3MvwBAI4BWdPw1ngbgOHR8OlTvvJZGFMv56DjUWQ9gnfN1WVzx8Cvj32dsuc28Tu2LV1AQkRV4BQURWYHFjoiskFGxi/vyL6KwMLfzT9prds4lMlsATEDHougqAFNUdVNw4RFFj7mdnwoz+F73EhkAEJEPL5HxTYjeUqwl6JvBlBSUZuzZrXwGhR/mdo46hAM4rC2dneSdUbHr7BKZsV19Qwn6YqyMz2BKCsrz+vT2uGPIYsztHLVCa33HMil2KV0iIyLTAUwHgBL0yWA6osgwt/NQJh9QpHSJjKpWq2qVqlYVoTiD6Ygiw9zOQ5kUu2y8RIYoCMztPJT2YayqtonIDQD+AqAAwDxV3RhYZEQxYW7np0zW7KCqzwJ4NqBYiLIGczv/8AoKIrICix0RWYHFjoiswGJHRFZgsSMiK7DYEZEVWOyIyAosdkRkBRY7IrICix0RWYHFjoiskNG1sbms/cKzjf4N1U+57QcrTw59/uZrxhn9Y9ftdtvtm7eGPj9RT+396qfc9orZDxpjI++f4bZPvHulMaZtbeEGliLu2RGRFVjsiMgK1h7Gbr/EvLNsacEHkc7/3ucPG/3W67y/O6WXRxoKUacKy08w+j/54a99t9008wG3fek9nzbGtLk52MDSxD07IrICix0RWYHFjoisYNWanRT1dtsXXbQuvkAAHP1KidG/etpf3fbSY4cYY+1790USE1GipktOMvoX92n13fbs1de47YEfbAktpkxwz46IrMBiR0RWsOowtvkK76qJe8rvNcZG/P4Gt12JFaHH0tLffMD8jf1fd9vLjh5hbszDWIpArz59jP4lN76Y8vcWP9Hf66j6bxgj7tkRkRVY7IjICix2RGSFvF6z0/NGGf377/4vt/3ofvNj9VNv9z4ubw81qg6funhDBLMQpa7lXHOt+KeD5vpu+88j5uWOxzz+91BiClK3e3YiMk9EmkRkQ8J7pSKyRETqndf+Xf0MomzE3LZLKoexNQAmJr03C0CtqlYCqHX6RLmmBsxta3R7GKuqy0VkaNLbkwBc4LTnA1gG4LYgAwvCnu/+0+gPKfRuIvidb3/eGCvasyb0eArLjnfb/33in42xVuXyadRyObfD8NaVBSlv++X6yUnvvBtoLGFI91/YYFVtBADndVBwIRHFirmdp0L/gEJEpgOYDgAl6NPN1kS5g7mdW9Lds9spImUA4Lw2+W2oqtWqWqWqVUUo9tuMKFswt/NUunt2iwFMBTDbeV0UWEQZ+sc3vIeC/PaMXxpj/7PvTLdd9Hz4a3TJNt1Z4bZb1TzBZeq2z7nt9qZdkcVEH5G1uR22z49+tcvxfUcOuu3WHw82xnrlw5qdiCwA8DKAU0SkQUSmoSMRJohIPYAJTp8opzC37ZLKp7FTfIbGBxwLUaSY23bJuysoek32nr96QqG5jjL3ce+UqiF4KfRYCk47xeg/Ov5ht92i5o0Q354z3G33bQn/ritEANBy2Wi3fV/5I11u25Dw+Ndef30lrJBCw5O7iMgKLHZEZAUWOyKyQs6v2RUMHGj0bx/+J99th9wV/jpdotdnHGv0q4q9003u3zPSGOu7kOt0FL2do4tS3vYLf7zZbUdxN++gcc+OiKzAYkdEVsj5w1jpYz5/9ZI+3sNpxqz6qjF2POoiielDA4a+7zv22FtV5rbIzmdtUn7rfdYe37G6w+Zdg069xzutK4ob3AaNe3ZEZAUWOyKyAosdEVkh59fsjry/1+j/ZJf3IOyvfGK1Mba87BNuu63xvVDiKTzJu7PJ30Y9kTTq/W05+PcBSWNcs6PwHbp8jNFfPfrBhJ55p+LNreZ9S9u3vBFWWJHgnh0RWYHFjoiswGJHRFbI/TW75maj/787TnXbL4x63Bhr/OPHvLGHP4V07B2pRr/f0H1Gf9wJ27zYcMT354j6DhGF5uAAc12uSPyfKHbrmiuN/jCsDyWmqHDPjoiswGJHRFbI+cPYZP3v8C4f++yPzbtu/+70Grd9949eTuvnr24xd/vbk/5eVPU+nNAT359z4r2vGX3/A16i4LRM3us7lnx52JBfp35HlFzAPTsisgKLHRFZgcWOiKyQd2t2WOmthX3sMnPougtudNt7K9N7gvtxj3S91rfjmdPc9pqxNb7bJZ8yQxSWguHeZZKrRz+aPOq2nvvgdGMkjgfJh4l7dkRkBRY7IrJC/h3GdqFg2Vq3fdyycOY4uO1orzPWfzs9b5TRl7+tCyUeop0Xencv6eqKifuWTjD6ufhQna50u2cnIhUislRE6kRko4jc5LxfKiJLRKTeee0ffrhEwWFu2yWVw9g2ALeo6ggA4wDMFJGRAGYBqFXVSgC1Tp8olzC3LdJtsVPVRlVd67SbAdQBKAcwCcB8Z7P5ACaHFCNRKJjbdunRmp2IDAVwFoAVAAaraiPQkTQiMqir77VGwhVivbr4W8I1uuySz7l9qNT/ssU1Ld7ljSPubjDG2kKLKB4pfxorIv0ALARws6ru78H3TReR1SKyuhUt6cRIFCrmth1SKnYiUoSOZHhMVZ9x3t4pImXOeBmAps6+V1WrVbVKVauKkN6JvERhYW7bo9vDWBERAHMB1KnqnIShxQCmApjtvC4KJcJck3BTzq5u3knxsyW3B120w3ds8f6z3Hb7rt2+2+WDVNbszgNwHYDXRGSd89730JEIT4nINABvA7gqlAiJwsPctki3xU5VX4T/jdnGBxsOUXSY23bh5WJEZAWrLheLwpES/3W6Xe38xI7CJ8XmhyWTTnjVd9t/HO7ntrUlv/OTe3ZEZAUWOyKyAg9jA/boxIfcdt1h85B2Ss2tbvtEvBRZTGSZ9najW113vtu++dxtxtiyd0522+XYGGpYceOeHRFZgcWOiKzAYkdEVuCaXcDufOuLbvvAA+XG2IkLuU5H4dM2834lQ2cdcNsjfn6dMSbrjoYtuGdHRFZgsSMiK/AwNmjjvRsg9kVDFxsSRaN961tu+0SLb2nAPTsisgKLHRFZgcWOiKzAYkdEVmCxIyIrsNgRkRVY7IjICix2RGQFFjsisgKLHRFZQVS1+62CmkxkF4DtAAYAyJYn8toay0mqOjCiufKek9sHkD25BNiZ2755HWmxcycVWa2qVZFP3AnGQkHJtt9fNsWTDbHwMJaIrMBiR0RWiKvYVcc0b2cYCwUl235/2RRP7LHEsmZHRBQ1HsYSkRUiLXYiMlFENovIVhGZFeXczvzzRKRJRDYkvFcqIktEpN557R9RLBUislRE6kRko4jcFGc8lJk4c5t5nZrIip2IFAC4H8ClAEYCmCIiI6Oa31EDYGLSe7MA1KpqJYBapx+FNgC3qOoIAOMAzHT+f8QVD6UpC3K7BszrbkW5ZzcGwFZVfVNVDwN4AsCkCOeHqi4H8H7S25MAzHfa8wFMjiiWRlVd67SbAdQBKI8rHspIrLnNvE5NlMWuHMA7Cf0G5724DVbVRqDjFwVgUNQBiMhQAGcBWJEN8VCPZWNux55H2ZbXURY76eQ96z8KFpF+ABYCuFlV98cdD6WFuZ0kG/M6ymLXAKAioT8EwLsRzu9np4iUAYDz2hTVxCJShI6EeExVn4k7HkpbNuY28zpJlMVuFYBKERkmIr0BXAtgcYTz+1kMYKrTngpgURSTiogAmAugTlXnxB0PZSQbc5t5nUxVI/sCcBmALQDeAPD9KOd25l8AoBFAKzr+Gk8DcBw6Ph2qd15LI4rlfHQc6qwHsM75uiyuePiV8e8zttxmXqf2xSsoiMgKvIKCiKzAYkdEVmCxIyIrsNgRkRVY7IjICix2RGQFFjsisgKLHRFZ4f8BaMAYaCfRwFUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 手动导入预测数据\n",
    "from tensorflow.keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data() \n",
    "\n",
    "# 取前4张图片\n",
    "test_images = test_images[2:6]\n",
    "test_labels = test_labels[2:6]\n",
    "print('图片原始格式:', test_images.shape)\n",
    "print('标签：', test_labels)\n",
    "\n",
    "# 图片格式转换成tensor格式\n",
    "test_images = torch.tensor(test_images)\n",
    "print('图片转换成tensor格式后：', test_images.shape)\n",
    "\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2,i+1)\n",
    "    plt.imshow(test_images[i])\n",
    "plt.show\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a0208ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 28, 28])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, TensorDataset, DataLoader\n",
    "\n",
    "# 图片类型转换成FloatTensor(预测和训练数据类型一致)\n",
    "test_images = test_images.type(torch.FloatTensor)/255\n",
    "\n",
    "print(test_images.shape)\n",
    "print(test_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72140cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "添加通道维度 torch.Size([4, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# 为图片格式添加通道维\n",
    "pic = test_images[0:4]\n",
    "\n",
    "pic = pic.reshape(4,1,28,28)\n",
    "print('添加通道维度', pic.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d6ee79",
   "metadata": {},
   "source": [
    "#### 方法一模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3e81d8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -8.4236,   8.2486,  -1.8376,  -6.6493,   0.0400,  -6.6488,  -4.3144,\n",
      "          -5.2530,  -1.7184,  -6.1824],\n",
      "        [ 11.8111, -13.9780,   0.6713,  -7.1739,  -4.0893,  -9.0231,  -0.4696,\n",
      "          -9.4936,  -2.8910,  -3.7154],\n",
      "        [ -8.5749, -12.0176,  -5.6263,  -8.8580,  11.6805, -14.2282, -10.1831,\n",
      "          -6.0848,  -6.8095,   0.3448],\n",
      "        [ -7.2165,   8.5743,  -1.8384,  -9.5966,   1.1469,  -7.3106,  -5.4645,\n",
      "          -2.2575,  -1.0350,  -5.6951]], grad_fn=<AddmmBackward>)\n",
      "------\n",
      "torch.return_types.max(\n",
      "values=tensor([ 8.2486, 11.8111, 11.6805,  8.5743], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([1, 0, 4, 1]))\n"
     ]
    }
   ],
   "source": [
    "# 预测模式\n",
    "\n",
    "output = model(pic)\n",
    "print(output)\n",
    "\n",
    "print('------')\n",
    "# 按行找到一行内值最大的列号\n",
    "prediction = torch.max(output, dim=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50953cf",
   "metadata": {},
   "source": [
    "#### 方法二模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "918258ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -8.4236,   8.2486,  -1.8376,  -6.6493,   0.0400,  -6.6488,  -4.3144,\n",
      "          -5.2530,  -1.7184,  -6.1824],\n",
      "        [ 11.8111, -13.9780,   0.6713,  -7.1739,  -4.0893,  -9.0231,  -0.4696,\n",
      "          -9.4936,  -2.8910,  -3.7154],\n",
      "        [ -8.5749, -12.0176,  -5.6263,  -8.8580,  11.6805, -14.2282, -10.1831,\n",
      "          -6.0848,  -6.8095,   0.3448],\n",
      "        [ -7.2165,   8.5743,  -1.8384,  -9.5966,   1.1469,  -7.3106,  -5.4645,\n",
      "          -2.2575,  -1.0350,  -5.6951]], grad_fn=<AddmmBackward>)\n",
      "------\n",
      "torch.return_types.max(\n",
      "values=tensor([ 8.2486, 11.8111, 11.6805,  8.5743], grad_fn=<MaxBackward0>),\n",
      "indices=tensor([1, 0, 4, 1]))\n"
     ]
    }
   ],
   "source": [
    "output = model_2(pic)\n",
    "print(output)\n",
    "\n",
    "print('------')\n",
    "# 按行找到一行内值最大的列号\n",
    "prediction = torch.max(output, dim=1)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b90a72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
